Quantum metrology promises greater sensitivity for optical phase measurements
than could ever be achieved classically. Here we present a theory of the phase
sensitivity for the general case where the detection probability is given by an
$N$ photon interference fringe. We find that the phase sensitivity has a
complex dependence on both the intrinsic efficiency of detection $\eta$ and the
interference fringe visibility $V$. Most importantly, the phase that gives
maximum phase sensitivity is in general not the same as the phase at which the
slope of the interference fringe is a maximum, as has previously been assumed.
We determine the parameter range where quantum enhanced sensitivity can be
achieved. In order to illustrate these theoretical results, we perform a four
photon experiment with $\eta=3/4$ and $V=82\pm6$% (an extension of our previous
work [Science \textbf{316}, 726 (2007)]) and find a phase sensitivity 1.3 times
greater than the standard quantum limit at a phase different to that which
gives maximum slope of the interference fringe.