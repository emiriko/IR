The authors are doing the readers of Statistical Science a true service with
a well-written and up-to-date overview of boosting that originated with the
seminal algorithms of Freund and Schapire. Equally, we are grateful for
high-level software that will permit a larger readership to experiment with, or
simply apply, boosting-inspired model fitting. The authors show us a world of
methodology that illustrates how a fundamental innovation can penetrate every
nook and cranny of statistical thinking and practice. They introduce the reader
to one particular interpretation of boosting and then give a display of its
potential with extensions from classification (where it all started) to least
squares, exponential family models, survival analysis, to base-learners other
than trees such as smoothing splines, to degrees of freedom and regularization,
and to fascinating recent work in model selection. The uninitiated reader will
find that the authors did a nice job of presenting a certain coherent and
useful interpretation of boosting. The other reader, though, who has watched
the business of boosting for a while, may have quibbles with the authors over
details of the historic record and, more importantly, over their optimism about
the current state of theoretical knowledge. In fact, as much as ``the
statistical view'' has proven fruitful, it has also resulted in some ideas
about why boosting works that may be misconceived, and in some recommendations
that may be misguided. [arXiv:0804.2752]