Calibration errors in the response function of a gravitational wave detector
degrade its ability to detect and then to measure the properties of any
detected signals. This paper derives the needed levels of calibration accuracy
for each of these data-analysis tasks. The levels derived here are optimal in
the sense that lower accuracy would result in missed detections and/or a loss
of measurement precision, while higher accuracy would be made irrelevant by the
intrinsic noise level of the detector. Calibration errors affect the
data-analysis process in much the same way as errors in theoretical waveform
templates. The optimal level of calibration accuracy is expressed therefore as
a joint limit on modeling and calibration errors: increased accuracy in one
reduces the accuracy requirement in the other.