We present the results of a series of radiation-MHD simulations of a local
patch of an accretion disk, with fixed vertical gravity profile but with
different surface mass densities and a broad range of radiation to gas pressure
ratios. Each simulation achieves a thermal equilibrium that lasts for many
cooling times. After averaging over times long compared to a cooling time, we
find that the vertically integrated stress is approximately proportional to the
vertically-averaged total thermal (gas plus radiation) pressure. We map
out--for the first time on the basis of explicit physics--the thermal
equilibrium relation between stress and surface density: the stress decreases
(increases) with increasing surface mass density when the simulation is
radiation (gas) pressure dominated. The dependence of stress on surface mass
density in the radiation pressure dominated regime suggests the possibility of
a Lightman-Eardley inflow instability, but global simulations or shearing box
simulations with much wider radial boxes will be necessary to confirm this and
determine its nonlinear behavior.