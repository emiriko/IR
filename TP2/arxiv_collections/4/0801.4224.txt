In this paper we introduce objective proper prior distributions for
hypothesis testing and model selection based on measures of divergence between
the competing models; we call them divergence based (DB) priors. DB priors have
simple forms and desirable properties, like information (finite sample)
consistency; often, they are similar to other existing proposals like the
intrinsic priors; moreover, in normal linear models scenarios, they exactly
reproduce Jeffreys-Zellner-Siow priors. Most importantly, in challenging
scenarios such as irregular models and mixture models, the DB priors are well
defined and very reasonable, while alternative proposals are not. We derive
approximations to the DB priors as well as MCMC and asymptotic expressions for
the associated Bayes factors.