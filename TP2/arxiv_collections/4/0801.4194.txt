We develop a statistical mechanical interpretation of algorithmic information
theory by introducing the notion of thermodynamic quantities, such as free
energy, energy, statistical mechanical entropy, and specific heat, into
algorithmic information theory. We investigate the properties of these
quantities by means of program-size complexity from the point of view of
algorithmic randomness. It is then discovered that, in the interpretation, the
temperature plays a role as the compression rate of the values of all these
thermodynamic quantities, which include the temperature itself. Reflecting this
self-referential nature of the compression rate of the temperature, we obtain
fixed point theorems on compression rate.