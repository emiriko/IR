Previous spreadsheet inspection experiments have had human subjects look for
seeded errors in spreadsheets. In this study, subjects attempted to find errors
in human-developed spreadsheets to avoid the potential artifacts created by
error seeding. Human subject success rates were compared to the successful
rates for error-flagging by spreadsheet static analysis tools (SSATs) applied
to the same spreadsheets. The human error detection results were comparable to
those of studies using error seeding. However, Excel Error Check and
Spreadsheet Professional were almost useless for correctly flagging natural
(human) errors in this study.