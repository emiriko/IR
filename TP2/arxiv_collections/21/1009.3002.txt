Most of physical experiments are usually described as repeated measurements
of some random variables. The experimental data registered by on-line computers
form time series of outcomes. The frequencies of different outcomes are
compared with the probabilities provided by the algorithms of quantum theory
(QT). In spite of statistical predictions of QT a claim was made that the
theory provided the most complete description of the data and of the underlying
physical phenomena. This claim could be easily rejected if some fine
structures, averaged out in standard statistical descriptive analysis, were
found in the time series of experimental data. To search for these structures
one has to use more subtle statistical tools which were developed to study time
series produced by various stochastic processes. In this talk we review some of
these tools. As an example we show how the standard descriptive statistical
analysis of the data is unable to reveal a fine structure in a simulated sample
of AR(2) stochastic process. We emphasize once again that the violation of Bell
inequalities gives no information on the completeness or the non locality of
QT. The appropriate way to test the completeness of quantum theory is to search
for fine structures in time series of experimental data by means of the purity
tests or by studying the autocorrelation and partial autocorrelation functions.