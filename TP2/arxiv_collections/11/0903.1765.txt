An important tool to quantify the likeness of two probability measures are
f-divergences, which have seen widespread application in statistics and
information theory. An example is the total variation, which plays an
exceptional role among the f-divergences. It is shown that every f-divergence
is bounded from below by a monotonous function of the total variation. Under
appropriate regularity conditions, this function is shown to be monotonous.
  Remark: The proof of the main proposition is relatively easy, whence it is
highly likely that the result is known. The author would be very grateful for
any information regarding references or related work.