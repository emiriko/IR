We study repeated games where players use an exponential learning scheme in
order to adapt to an ever-changing environment. If the game's payoffs are
subject to random perturbations, this scheme leads to a new stochastic version
of the replicator dynamics that is quite different from the "aggregate shocks"
approach of evolutionary game theory. Irrespective of the perturbations'
magnitude, we find that strategies which are dominated (even iteratively)
eventually become extinct and that the game's strict Nash equilibria are
stochastically asymptotically stable. We complement our analysis by
illustrating these results in the case of congestion games.