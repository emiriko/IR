We analyze a suite of thin sheet magnetohydrodynamical simulations based on
the formulation of Basu, Ciolek, Dapp & Wurster. These simulations allow us to
examine the observational consequences to a star-forming region of varying the
input level of turbulence (between thermal and a Mach number of 4) and the
initial magnetic field strength corresponding to a range of mass to flux ratios
between subcritical (mu_0=0.5) and supercritical (mu_0=10). The input
turbulence is allowed to decay over the duration of the simulation. We compare
the measured observable quantities with those found from surveying the Perseus
molecular cloud. We find that only the most turbulent of simulations (high Mach
number and weak magnetic field) have sufficient large-scale velocity dispersion
(at ~1 pc) to match that observed across extinction regions in Perseus.
Generally, the simulated core (~0.02 pc) and line of sight velocity dispersions
provide a decent match to observations. The motion between the simulated core
and its local environment, however, is far too large in simulations with high
large-scale velocity dispersion.