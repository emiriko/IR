We study optimal solutions to an abstract optimization problem for measures,
which is a generalization of classical variational problems in information
theory and statistical physics. In the classical problems, information and
relative entropy are defined using the Kullback-Leibler divergence, and for
this reason optimal measures belong to a one-parameter exponential family.
Measures within such a family have the property of mutual absolute continuity.
Here we show that this property characterizes other families of optimal
positive measures if a functional representing information has a strictly
convex dual. Mutual absolute continuity of optimal probability measures allows
us to strictly separate deterministic and non-deterministic Markov transition
kernels, which play an important role in theories of decisions, estimation,
control, communication and computation. We show that deterministic transitions
are strictly sub-optimal, unless information resource with a strictly convex
dual is unconstrained. For illustration, we construct an example where, unlike
non-deterministic, any deterministic kernel either has negatively infinite
expected utility (unbounded expected error) or communicates infinite
information.