As no heat effect and mechanical work are observed, we have a simple
experimental resolution of the Gibbs paradox: both the thermodynamic entropy of
mixing and the Gibbs free energy change are zero during the formation of any
ideal mixtures. Information loss is the driving force of these spontaneous
processes. Information is defined as the amount of the compressed data.
Information losses due to dynamic motion and static symmetric structure
formation are defined as two kinds of entropies - dynamic entropy and static
entropy, respectively. There are three laws of information theory, where the
first and the second laws are analogs of the two thermodynamic laws. However,
the third law of information theory is different: for a solid structure of
perfect symmetry (e.g., a perfect crystal), the entropy (static entropy for
solid state) S is the maximum. More generally, a similarity principle is set
up: if all the other conditions remain constant, the higher the similarity
among the components is, the higher the value of entropy of the mixture (for
fluid phases) or the assemblage (for a static structure or a system of
condensed phases) or any other structure (such as quantum states in quantum
mechanics) will be, the more stable the mixture or the assemblage will be, and
the more spontaneous the process leading to such a mixture or an assemblage or
a chemical bond will be.