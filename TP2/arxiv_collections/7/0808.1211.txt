Over two decades ago a "quite revolution" overwhelmingly replaced
knowledgebased approaches in natural language processing (NLP) by quantitative
(e.g., statistical, corpus-based, machine learning) methods. Although it is our
firm belief that purely quantitative approaches cannot be the only paradigm for
NLP, dissatisfaction with purely engineering approaches to the construction of
large knowledge bases for NLP are somewhat justified. In this paper we hope to
demonstrate that both trends are partly misguided and that the time has come to
enrich logical semantics with an ontological structure that reflects our
commonsense view of the world and the way we talk about in ordinary language.
In this paper it will be demonstrated that assuming such an ontological
structure a number of challenges in the semantics of natural language (e.g.,
metonymy, intensionality, copredication, nominal compounds, etc.) can be
properly and uniformly addressed.