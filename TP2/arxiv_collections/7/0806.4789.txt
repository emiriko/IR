Intrinsic computation refers to how dynamical systems store, structure, and
transform historical and spatial information. By graphing a measure of
structural complexity against a measure of randomness, complexity-entropy
diagrams display the range and different kinds of intrinsic computation across
an entire class of system. Here, we use complexity-entropy diagrams to analyze
intrinsic computation in a broad array of deterministic nonlinear and linear
stochastic processes, including maps of the interval, cellular automata and
Ising spin systems in one and two dimensions, Markov chains, and probabilistic
minimal finite-state machines. Since complexity-entropy diagrams are a function
only of observed configurations, they can be used to compare systems without
reference to system coordinates or parameters. It has been known for some time
that in special cases complexity-entropy diagrams reveal that high degrees of
information processing are associated with phase transitions in the underlying
process space, the so-called ``edge of chaos''. Generally, though,
complexity-entropy diagrams differ substantially in character, demonstrating a
genuine diversity of distinct kinds of intrinsic computation.