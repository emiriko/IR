We propose a new sparsity-smoothness penalty for high-dimensional generalized
additive models. The combination of sparsity and smoothness is crucial for
mathematical theory as well as performance for finite-sample data. We present a
computationally efficient algorithm, with provable numerical convergence
properties, for optimizing the penalized likelihood. Furthermore, we provide
oracle results which yield asymptotic optimality of our estimator for high
dimensional but sparse additive models. Finally, an adaptive version of our
sparsity-smoothness penalized approach yields large additional performance
gains.