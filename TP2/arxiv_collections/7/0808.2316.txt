We present a gradient-based algorithm for unconstrained minimization derived
from iterated linear change of basis. The new method is equivalent to linear
conjugate gradient in the case of a quadratic objective function. In the case
of exact line search it is a secant method. In practice, it performs comparably
to BFGS and DFP and is sometimes more robust.