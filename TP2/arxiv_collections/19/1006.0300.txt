So far, there have been plenty of literatures on the metric in the space of
probability distributions and quantum states. As for channels, however, only a
little had been known. In this paper, we impose monotonicity by concatenation
of channels before and after the given channel families, and invariance by
tensoring identity channels. Under these axioms, we identify the largest and
the smallest metrics. Also, we studied asymptotic theory of metric in parallel
and adaptive repetition settings, and applied them to the study of channel
estimation. First we express the achievable lower bound of the mean square
error (MSE) of an estimate by a monotone channel metric, and show this equals
O(1/n) for noisy channels, where n is the number of times of channel use. This
result shows Heisenberg rate, or O(1/n^2)-rate of the MSE observed in case of
estimation of unitary, collapses with very small arbitrary noise.