Two familiar notions of correlation are rediscovered as extreme operating
points for simulating a discrete memoryless channel, in which a channel output
is generated based only on a description of the channel input. Wyner's "common
information" coincides with the minimum description rate needed. However, when
common randomness independent of the input is available, the necessary
description rate reduces to Shannon's mutual information. This work
characterizes the optimal tradeoff between the amount of common randomness used
and the required rate of description.