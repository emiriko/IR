Nearest neighbor cells in $R^d,d\in\mathbb{N}$, are used to define
coefficients of divergence ($\phi$-divergences) between continuous multivariate
samples. For large sample sizes, such distances are shown to be asymptotically
normal with a variance depending on the underlying point density. In $d=1$,
this extends classical central limit theory for sum functions of spacings. The
general results yield central limit theorems for logarithmic $k$-spacings,
information gain, log-likelihood ratios and the number of pairs of sample
points within a fixed distance of each other.