We present a dynamic programming-based solution to a stochastic optimal
control problem up to a hitting time for a discrete-time Markov control
process. Firstly, we determine an optimal control policy to steer the process
toward a compact target set while simultaneously minimizing an expected
discounted cost. We then provide a rolling-horizon strategy for approximating
the optimal policy, together with quantitative characterization of its
sub-optimality with respect to the optimal policy. Finally, we address related
issues of asymptotic discount-optimality of the value-iteration policy. Both
the state and action spaces are assumed to be Polish.