In this article we study the asymptotic predictive optimality of a model
selection criterion based on the cross-validatory predictive density, already
available in the literature. For a dependent variable and associated
explanatory variables, we consider a class of linear models as approximations
to the true regression function. One selects a model among these using the
criterion under study and predicts a future replicate of the dependent variable
by an optimal predictor under the chosen model. We show that for squared error
prediction loss, this scheme of prediction performs asymptotically as well as
an oracle, where the oracle here refers to a model selection rule which
minimizes this loss if the true regression were known.