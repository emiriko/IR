In 1997, Z.Zhang and R.W.Yeung found the first example of a conditional
information inequality in four variables that is not "Shannon-type". This
linear inequality for entropies is called conditional (or constraint) since it
holds only under condition that some linear equations are satisfied for the
involved entropies. Later, the same authors and other researchers discovered
several unconditional information inequalities that do not follow from
Shannon's inequalities for entropy.
  In this paper we show that some non Shannon-type conditional inequalities are
"essentially" conditional, i.e., they cannot be extended to any unconditional
inequality. We prove one new essentially conditional information inequality for
Shannon's entropy and discuss conditional information inequalities for
Kolmogorov complexity.