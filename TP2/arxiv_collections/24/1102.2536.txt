In this paper we establish lower bounds on information divergence from a
distribution to certain important classes of distributions as Gaussian,
exponential, Gamma, Poisson, geometric, and binomial. These lower bounds are
tight and for several convergence theorems where a rate of convergence can be
computed, this rate is determined by the lower bounds proved in this paper.
General techniques for getting lower bounds in terms of moments are developed.