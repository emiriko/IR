Recent experiments on ultra slow light in strongly dispersive media by
several research groups reporting slowing down of the optical pulses down to
speeds of a few metres per second encourage us to examine the intriguing
possibility of detecting a deflection or fall of the ultra slow light under
Earth's gravity, i.e., on the laboratory length scale. In the absence of a
usable general relativistic theory of light waves propagating in such a
strongly dispersive optical medium in the presence of a gravitational field, we
present a geometrical optics based derivation that combines {\it the effective
gravitational refractive index} additively with the usual optical dispersion.
It gives a deflection, or the vertical fall $\Delta$ for a horizontal traversal
$L$ as \[ \Delta = \frac{L^2}{2}\big(\frac{R_{\oplus G}}{R_\oplus^2}\big) n_g
\big(\frac{1}{1+n_g\frac{R_{\oplus G}}{R_\oplus}}\big), \] where $R_{\oplus
G}/R_\oplus$ is the ratio of the gravitational Earth radius($R_{\oplus G}$) to
its geometrical radius $R_\oplus$, and $n_g$ is the group refractive index of
the strongly dispersive optical medium. The expression is essentailly that for
the Newtonian fall of an object projected horizontally with the group speed
$v_g=c/n_g$, and is tunable refractively through the index $n_g$. For $L \sim 1
m$ and $n_g = c/v_g \sim 10^8$ (corresponding to the ultra-slow pulse speed
$\sim few \times 1 ms^{-1}$), we obtain a fall $\Delta \sim 1 \mu m$, that
should be measurable $-$ in particular through its sensitive dependence on the
frequency that tunes $n_g$.