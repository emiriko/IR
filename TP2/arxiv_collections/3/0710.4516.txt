We show that the predictability of letters in written English texts depends
strongly on their position in the word. The first letters are usually the least
easy to predict. This agrees with the intuitive notion that words are well
defined subunits in written languages, with much weaker correlations across
these units than within them. It implies that the average entropy of a letter
deep inside a word is roughly 4 times smaller than the entropy of the first
letter.