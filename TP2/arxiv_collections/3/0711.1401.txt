The pace of progress in the fields of Evolutionary Computation and Machine
Learning is currently limited -- in the former field, by the improbability of
making advantageous extensions to evolutionary algorithms when their capacity
for adaptation is poorly understood, and in the latter by the difficulty of
finding effective semi-principled reductions of hard real-world problems to
relatively simple optimization problems. In this paper we explain why a theory
which can accurately explain the simple genetic algorithm's remarkable capacity
for adaptation has the potential to address both these limitations. We describe
what we believe to be the impediments -- historic and analytic -- to the
discovery of such a theory and highlight the negative role that the building
block hypothesis (BBH) has played. We argue based on experimental results that
a fundamental limitation which is widely believed to constrain the SGA's
adaptive ability (and is strongly implied by the BBH) is in fact illusionary
and does not exist. The SGA therefore turns out to be more powerful than it is
currently thought to be. We give conditions under which it becomes feasible to
numerically approximate and study the multivariate marginals of the search
distribution of an infinite population SGA over multiple generations even when
its genomes are long, and explain why this analysis is relevant to the riddle
of the SGA's remarkable adaptive abilities.