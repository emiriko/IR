The star formation rate (SFR) is a key parameter in the study of galaxy
evolution. The accuracy of SFR measurements at z~2 has been questioned
following a disagreement between observations and theoretical models. The
latter predict SFRs at this redshift that are typically a factor 4 or more
lower than the measurements. We present star-formation rates based on
calorimetric measurements of the far-infrared (FIR) luminosities for massive
1.5<z<2.5, normal star-forming galaxies (SFGs), which do not depend on
extinction corrections and/or extrapolations of spectral energy distributions.
The measurements are based on observations in GOODS-N with the Photodetector
Array Camera & Spectrometer (PACS) onboard Herschel, as part of the PACS
Evolutionary Probe (PEP) project, that resolve for the first time individual
SFGs at these redshifts at FIR wavelengths. We compare FIR-based SFRs to the
more commonly used 24 micron and UV SFRs. We find that SFRs from 24 micron
alone are higher by a factor of ~4-7.5 than the true SFRs. This overestimation
depends on luminosity: gradually increasing for log L(24um)>12.2 L_sun. The
SFGs and AGNs tend to exhibit the same 24 micron excess. The UV SFRs are in
closer agreement with the FIR-based SFRs. Using a Calzetti UV extinction
correction results in a mean excess of up to 0.3 dex and a scatter of 0.35 dex
from the FIR SFRs. The previous UV SFRs are thus confirmed and the mean excess,
while narrowing the gap, is insufficient to explain the discrepancy between the
observed SFRs and simulation predictions.