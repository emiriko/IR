The paper makes the observation that all orders of information entropy are
equal in signals composed of repeating units of distinct symbols where the
units can be classified as a member of a symmetry group. This leads to an
improved metric for measuring the information content of higher order entropies
in data such as text, signals, or genetics and another measure of similarity to
compare the incremental information content across entropy orders when
comparing data of different sizes and symbol sets or when comparing entire
sequences.