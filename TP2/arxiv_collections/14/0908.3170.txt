I present a new approach for the interpretation of reaction time (RT) data
from behavioral experiments. From a physical perspective, the entropy of the RT
distribution provides a model-free estimate of the amount of processing
performed by the cognitive system. In this way, the focus is shifted from the
conventional interpretation of individual RTs being either long or short, into
their distribution being more or less complex in terms of entropy. The new
approach enables the estimation of the cognitive processing load without
reference to the informational content of the stimuli themselves, thus
providing a more appropriate estimate of the cognitive impact of different
sources of information that are carried by experimental stimuli or tasks. The
paper introduces the formulation of the theory, followed by an empirical
validation using a database of human RTs in lexical tasks (visual lexical
decision and word naming). The results show that this new interpretation of RTs
is more powerful than the traditional one. The method provides theoretical
estimates of the processing loads elicited by individual stimuli. These loads
sharply distinguish the responses from different tasks. In addition, it
provides upper-bound estimates for the speed at which the system processes
information. Finally, I argue that the theoretical proposal, and the associated
empirical evidence, provide strong arguments for an adaptive system that
systematically adjusts its operational processing speed to the particular
demands of each stimulus. This finding is in contradiction with Hick's law,
which posits a relatively constant processing speed within an experimental
context.