Software inspection is a necessary and important tool for software quality
assurance. Since it was introduced by Fagan at IBM in 1976, arguments exist as
to which method should be adopted to carry out the exercise, whether it should
be paper based or tool based, and what reading technique should be used on the
inspection document. Extensive works have been done to determine the
effectiveness of reviewers in paper based environment when using ad hoc and
checklist reading techniques. In this work, we take the software inspection
research further by examining whether there is going to be any significant
difference in defect detection effectiveness of reviewers when they use either
ad hoc or checklist reading techniques in a distributed groupware environment.
Twenty final year undergraduate students of computer science, divided into ad
hoc and checklist reviewers groups of ten members each were employed to inspect
a medium sized java code synchronously on groupware deployed on the Internet.
The data obtained were subjected to tests of hypotheses using independent T
test and correlation coefficients. Results from the study indicate that there
are no significant differences in the defect detection effectiveness, effort in
terms of time taken in minutes and false positives reported by the reviewers
using either ad hoc or checklist based reading techniques in the distributed
groupware environment studied.