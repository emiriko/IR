We study the effects of externally imposed turbulence on the thermal
properties of galaxy cluster cores, using three-dimensional numerical
simulations including magnetic fields, anisotropic thermal conduction, and
radiative cooling. The imposed "stirring" crudely approximates the effects of
galactic wakes, waves generated by galaxies moving through the intracluster
medium (ICM), and/or turbulence produced by a central active galactic nucleus.
The simulated clusters exhibit a strong bimodality. Modest levels of
turbulence, ~100 km/s (~10% of the sound speed), suppress the heat-flux-driven
buoyancy instability (HBI), resulting in an isotropically tangled magnetic
field and a quasi-stable, high entropy, thermal equilibrium with no cooling
catastrophe. Thermal conduction dominates the heating of the cluster core, but
turbulent mixing is critical because it suppresses the HBI and (to a lesser
extent) the thermal instability. Lower levels of turbulent mixing
(approximately less than 100 km/s) are insufficient to suppress the HBI,
rapidly leading to a thermal runaway and a cool-core cluster. Remarkably, then,
small fluctuations in the level of turbulence in galaxy cluster cores can
initiate transitions between cool-core (low entropy) and non cool-core (high
entropy) states.