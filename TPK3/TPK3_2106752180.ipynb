{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofk6UwM84k6g"
      },
      "source": [
        "# Tugas Pemrograman Kecil 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7Un7J8vyrcT"
      },
      "source": [
        "# Chapter 1: Embedding with SVD & LSA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42Yguq_64oAb"
      },
      "source": [
        "Pengantar Singular Value Decomposition (SVD) dan Latent Semantic Analysis (LSA)\n",
        "\n",
        "SVD adalah metode aljabar untuk memecah matriks menjadi komponen-komponen yang lebih sederhana. Metode ini memungkinkan kita untuk menyederhanakan data yang kompleks tanpa kehilangan informasi penting.\n",
        "\n",
        "LSA, di sisi lain, menggunakan SVD untuk menganalisis hubungan antara kata-kata dan dokumen dalam korpus. Hasil dari LSA dapat kita jadikan sebagai embedding, baik itu `document embeddings` atau `word embeddings`\n",
        "\n",
        "Dalam tutorial ini, kita akan mempelajari dasar-dasar matematika di balik SVD dan LSA, serta bagaimana menerapkannya menggunakan Python. Kita juga akan melihat bagaimana teknik-teknik ini dapat digunakan untuk mengurangi dimensi data dan menemukan hubungan semantik dalam teks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "tD0YlCH84nHA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu2rxAHuyQNd"
      },
      "source": [
        "## Singular Value Decomposition (SVD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_QbZ_YF5V95"
      },
      "source": [
        "   SVD adalah sebuah teknik dalam aljabar linier yang menguraikan sebuah matriks menjadi tiga matriks komponen. Untuk setiap matriks $A \\in \\mathbb{R}^{m \\times n}$, SVD menyatakannya sebagai:\n",
        "\n",
        "   $A = U \\Sigma V ^ T $\n",
        "\n",
        "   Di mana:\n",
        "   - $ U \\in \\mathbb{R}^{m \\times m}$ adalah matriks ortogonal yang berisi vektor singular kiri\n",
        "   - $\\Sigma \\in \\mathbb{R}^{m \\times n}$ adalah matriks diagonal dengan nilai tunggal\n",
        "   - $V^T \\in \\mathbb{R}^{n \\times n}$ adalah transpose dari matriks ortogonal yang berisi vektor singular kanan\n",
        "\n",
        "### Signifikansi Komponen SVD:\n",
        "   - Entri diagonal $\\Sigma$ (nilai singular) mewakili pentingnya setiap dimensi\n",
        "   - Nilai singular berhubungan dengan nilai eigen: $\\sigma_i = \\sqrt{\\lambda_i}$\n",
        "   - $\\lambda_i$ adalah nilai eigen dari $A^TA$ atau $AA^T$\n",
        "\n",
        "### Proses Komputasi SVD:\n",
        "   1. Bentuklah matriks covariance C\n",
        "   2. Hitung nilai eigen dan vektor eigen dari C\n",
        "   3. Dapatkan nilai-nilai singular: $\\sigma_i = \\sqrt{\\lambda_i}$\n",
        "   4. Buat V dari vektor-vektor eigen dari C\n",
        "   5. Hitung U: $U_i = \\frac{1}{\\sigma_i}AV_i$\n",
        "\n",
        "### SVD Truncated dan Low-Rank Approximation:\n",
        "   SVD memungkinkan reduksi dimensi dengan melakukan aproksimasi A dengan dimensi yang lebih sedikit:\n",
        "   \n",
        "   $A_k = \\sum_{i=1}^k \\sigma_i u_i v_i^T$\n",
        "   \n",
        "   k-rank approximation ini masih bisa menangkap fitur-fitur terpenting dari matriks asli."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQnawnf_rNoL"
      },
      "source": [
        "### Task 1\n",
        "Implementasikan SVD from scratch dengan hanya memanfaatkan numpy <font color='red'>**tanpa function SVD dari luar** </font>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGJJ6Sv6sN29"
      },
      "source": [
        "Feel free untuk mengikuti algoritma dibawah ini, kalau ada cara sendiri juga silakan.\n",
        "\n",
        "1. Deklarasi fungsi SVD:\n",
        "   - Input: Matriks A dan parameter opsional k (jumlah singular value yang akan dihitung)\n",
        "   - Ubah A menjadi array NumPy dan tentukan shapenya (n x m)\n",
        "   - Ubah k ke min(n, m) jika tidak ditentukan, atau min(k, n, m) jika ada\n",
        "\n",
        "2. Hitung matriks covariance:\n",
        "   - Jika n >= m, hitung C = A^T * A\n",
        "   - Jika n < m, hitung C = A * A^T\n",
        "\n",
        "3. Menghitung nilai eigen dan vektor eigen:\n",
        "   - Gunakan `np.linalg.eigh(C)` untuk menghitung nilai eigen dan vektor eigen dari C\n",
        "   - Urutkan nilai eigen dan vektor eigen secara descending\n",
        "\n",
        "4. Hitung nilai singular:\n",
        "   - Ambil akar kuadrat dari k nilai eigen pertama\n",
        "\n",
        "5. Hitung vektor-vektor singular kanan:\n",
        "   - Tetapkan V sebagai k kolom pertama dari vektor eigen yang telah diurutkan\n",
        "   - Jika n < m, update V dengan rumus: `V = A^T * V / singular_values` dan set U menjadi V.\n",
        "     - Return U, matriks diagonal dari singular values, dan V^T\n",
        "     - Lihat dibawah kenapa bisa begini kalau penasaran\n",
        "\n",
        "6. Hitung vektor singular kiri:\n",
        "   - Hitung U dengan `U = A^T * V / singular_values`\n",
        "\n",
        "7. Return U, matriks diagonal dari singular values, dan V^T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "rKgID2OS5Eb6"
      },
      "outputs": [],
      "source": [
        "def svd(A, k=None):\n",
        "    A = np.array(A, dtype=float)\n",
        "    n, m = A.shape\n",
        "    k = min(n, m) if k is None else min(k, n, m)\n",
        "\n",
        "    # TODO\n",
        "\n",
        "    # 2. Menghitung matrix covariance\n",
        "    if n >= m:\n",
        "      C = A.T @ A\n",
        "    else:\n",
        "      C = A @ A.T\n",
        "\n",
        "    # 3 .Menghitung vector eigen dan nilai eigen (labmda)\n",
        "    eigen_values, eigen_vectors = np.linalg.eigh(C)\n",
        "\n",
        "    idx = np.argsort(eigen_values)[::-1]\n",
        "    eigen_values = eigen_values[idx]\n",
        "    eigen_vectors = eigen_vectors[:, idx]\n",
        "\n",
        "    # 4. Hitung nilai singular\n",
        "    singular_values = np.sqrt(eigen_values[:k])\n",
        "\n",
        "    # 5. Hitung Vektor singular kanan\n",
        "    V = eigen_vectors[:, :k]\n",
        "\n",
        "    if n >= m:\n",
        "        U = (A @ V) / singular_values\n",
        "    else:\n",
        "        U = V\n",
        "        V = (A.T @ U) / singular_values\n",
        "\n",
        "    return U, np.diag(singular_values), V.T\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OeinspW5-e9"
      },
      "source": [
        "### Informasi tambahan untuk step 5\n",
        "\n",
        "Dalam SVD, kita mendekomposisi matriks A (n x m) menjadi U (n x k), S (k x k), dan V^T (k x m), di mana k adalah jumlah nilai singular yang kita hitung.\n",
        "\n",
        "Ketika n < m, kita awalnya menghitung vektor eigen dari AA^T alih-alih A^TA. Mari kita sebut vektor eigen ini U_init. Vektor eigen ini berkorespondensi dengan vektor singular kiri A, tapi bukan vektor singular kanan yang kita butuhkan untuk V.\n",
        "\n",
        "Berikut alasan mengapa rumus `V = A^T * V / singular_values` bekerja:\n",
        "\n",
        "1. Dalam SVD, kita memiliki hubungan: $AV = US$\n",
        "\n",
        "2. Bisa diapakan A^T: $A^TAV = A^TUS$\n",
        "\n",
        "3. Ingat bahwa V seharusnya adalah vektor eigen dari A^TA. Jadi, $A^TAV = V\\Sigma^2$, di mana $\\Sigma^2$ adalah matriks diagonal dari nilai eigen.\n",
        "\n",
        "4. Dari langkah 2 dan 3: $A^TUS = V\\Sigma^2$\n",
        "\n",
        "5. Bagi kedua sisi dengan $\\Sigma$ (matriks diagonal dari nilai singular): $(A^TU)\\Sigma^{-1} = V$\n",
        "\n",
        "6. Ini setara dengan rumus kita: $V = \\frac{A^T \\cdot U}{\\text{singular values}}$\n",
        "\n",
        "Dalam implementasi kita, awalnya kita menghitung U_init (vektor eigen dari AA^T) alih-alih U. Tapi U_init sebenarnya sama dengan U, jadi kita bisa menggunakannya sebagai pengganti U dalam rumus.\n",
        "\n",
        "Jadi, ketika kita menghitung $V = \\frac{A^T \\cdot U}{\\text{singular values}}$, kita secara efektif mengubah vektor singular kiri (yang kita hitung sebagai vektor eigen dari AA^T) menjadi vektor singular kanan.\n",
        "\n",
        "Pendekatan ini memungkinkan kita untuk menghitung V yang benar bahkan ketika kita mulai dengan AA^T alih-alih A^TA, memastikan bahwa SVD kita bekerja dengan benar untuk kedua kasus n >= m dan n < m."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ilvrA43yVx0"
      },
      "source": [
        "## Latent Semantic Analysis (LSA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_3Noqn-uXtE"
      },
      "source": [
        "   LSA menerapkan SVD untuk menganalisis hubungan antara dokumen dan term dalam korpus:\n",
        "   1. Membangun matriks istilah-dokumen A menggunakan pembobotan TF-IDF\n",
        "   2. Lakukan SVD pada\n",
        "   \n",
        "   $A = U\\Sigma V^T$\n",
        "\n",
        "   3. Reduksi dimensi dengan mengambil hanya k nilai/vektor singular teratas\n",
        "\n",
        "### TF-IDF\n",
        "   TF-IDF mengukur tingkat kepentingan term dalam sebuah dokumen relatif terhadap sebuah korpus:\n",
        "\n",
        "   $\\text{TF-IDF}(t,d,D) = \\text{TF}(t,d) \\cdot \\text{IDF}(t,D)$\n",
        "   \n",
        "   Dimana:\n",
        "\n",
        "   - $\\text{TF}(t,d) = \\frac{\\text{jumlah term t di dokumen d}}{\\text{total term di d}}$\n",
        "   - $\\text{IDF}(t,D) = \\log\\frac{\\text{total dokumen}}{\\text{dokumen yang mengandung t}}$\n",
        "\n",
        "### Menafsirkan Hasil LSA:\n",
        "   Setelah menerapkan SVD:\n",
        "   - Baris $U\\Sigma_k$ mewakili dokumen dalam ruang semantik yang telah direduksi\n",
        "   - Kolom $V^T$ mewakili terms dalam ruang ini\n",
        "   Kemiripan antara dokumen atau istilah dapat diukur dengan menggunakan kesamaan kosinus dalam ruang yang direduksi ini."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0eoqfPxuy05"
      },
      "source": [
        "### Task 2\n",
        "Implementasikan LSA dengan implementasi SVD kalian sebelumnya."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWOire3iu93U"
      },
      "source": [
        "Feel free untuk mengikuti algoritma dibawah ini, kalau ada cara sendiri juga silakan.\n",
        "\n",
        "1. Deklarasi fungsi LSA:\n",
        "   - Input: `documents` dan `num_topics`\n",
        "\n",
        "2. Lakukan vektorisasi TF-IDF:\n",
        "    - Silakan gunakan `TfidfVectorizer` dari `sklearn`\n",
        "\n",
        "3. Terapkan SVD ke matriks TF-IDF:\n",
        "    - Panggil fungsi SVS sebelumnya dengan matriks tfidf sebelumnya dan `k = num_topics`\n",
        "\n",
        "4. Hitung matriks:\n",
        "    - Matriks document-topic: U * S\n",
        "    - Matriks term-topic: V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "mAugcbdm5Qnk"
      },
      "outputs": [],
      "source": [
        "def lsa(documents, num_topics):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(documents).toarray()\n",
        "    terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # TODO\n",
        "    U, S, VT = svd(tfidf_matrix, num_topics)\n",
        "\n",
        "    doc_topic_matrix = U @ S\n",
        "    term_topic_matrix = VT\n",
        "\n",
        "    # U = U[:, :num_topics]\n",
        "    # S = np.diag(S[:num_topics])\n",
        "    # VT = VT[:num_topics, :]\n",
        "\n",
        "    # doc_topic_matrix = VT.T @ S\n",
        "\n",
        "    # term_topic_matrix = U @ S\n",
        "\n",
        "    return doc_topic_matrix, term_topic_matrix, terms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL94pJWzAKVf"
      },
      "source": [
        "Untuk mendapatkan embeddingnya, kita dapat menggunakan matriks yang dihasilkan oleh LSA. Jangan lupa untuk men-transpose term matriksnya."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "5vMrB9tVALM6",
        "outputId": "3025241c-1cdb-4772-9ea1-cf276fff916e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.65503505  0.08756422 -0.23535281]\n",
            " [-0.59749834  0.37524266 -0.13820511]\n",
            " [-0.42550279  0.30898413 -0.22702776]\n",
            " [-0.26376106  0.21932515  0.50295872]\n",
            " [-0.07958578  0.07452861  0.70868065]\n",
            " [-0.3018436  -0.08110509  0.45279548]\n",
            " [-0.3235538  -0.70737842 -0.01379984]\n",
            " [-0.39291898 -0.5706551  -0.01102955]\n",
            " [-0.28178756  0.00269099  0.05822144]]\n",
            "[[-0.01598763 -0.06461839 -0.08326329 -0.21075079 -0.06678949 -0.06308913\n",
            "  -0.08326329 -0.14520269 -0.15543487 -0.15543487 -0.01598763 -0.05856991\n",
            "  -0.09910895 -0.01598763 -0.06308913 -0.12490322 -0.06308913 -0.05856991\n",
            "  -0.08326329 -0.05856991 -0.05856991 -0.06461839 -0.06244244 -0.14520269\n",
            "  -0.06308913 -0.05856991 -0.09910895 -0.12490322 -0.05856991 -0.06461839\n",
            "  -0.33859892 -0.06461839 -0.25014487 -0.05794236 -0.05794236 -0.05794236\n",
            "  -0.06308913 -0.06308913 -0.06308913 -0.25392327 -0.08326329 -0.09910895\n",
            "  -0.1040468  -0.06461839 -0.08326329 -0.06461839 -0.14520269 -0.15543487\n",
            "  -0.09910895 -0.05794236 -0.01598763 -0.01598763 -0.05794236 -0.05794236\n",
            "  -0.06461839 -0.05794236 -0.14520269 -0.06461839 -0.06308913 -0.15543487\n",
            "  -0.05856991 -0.05794236 -0.06461839 -0.01598763 -0.05856991 -0.29356151\n",
            "  -0.09910895 -0.14520269 -0.03197526 -0.06308913 -0.09910895 -0.08326329\n",
            "  -0.22994107 -0.08326329 -0.25392327 -0.08326329 -0.09910895 -0.05856991]\n",
            " [ 0.01950736 -0.18407223 -0.15756187  0.10396918 -0.00217927 -0.02208755\n",
            "  -0.15756187  0.11881656  0.02707307  0.02707307  0.01950736  0.00072877\n",
            "   0.09377207  0.01950736 -0.02208755 -0.2885496  -0.02208755  0.00072877\n",
            "  -0.15756187  0.00072877  0.00072877 -0.18407223  0.0694987   0.11881656\n",
            "  -0.02208755  0.00072877  0.09377207 -0.2885496   0.00072877 -0.18407223\n",
            "  -0.20086966 -0.18407223  0.0764376   0.06277703  0.06277703  0.06277703\n",
            "  -0.02208755 -0.02208755 -0.02208755  0.1232207  -0.15756187  0.09377207\n",
            "  -0.1548548  -0.18407223 -0.15756187 -0.18407223  0.11881656  0.02707307\n",
            "   0.09377207  0.06277703  0.01950736  0.01950736  0.06277703  0.06277703\n",
            "  -0.18407223  0.06277703  0.11881656 -0.18407223 -0.02208755  0.02707307\n",
            "   0.00072877  0.06277703 -0.18407223  0.01950736  0.00072877  0.17600016\n",
            "   0.09377207  0.11881656  0.03901471 -0.02208755  0.09377207 -0.15756187\n",
            "   0.08928013 -0.15756187  0.1232207  -0.15756187  0.09377207  0.00072877]\n",
            " [ 0.19234496 -0.00372362 -0.00315784  0.1609925   0.27045558  0.12786637\n",
            "  -0.00315784 -0.04537783 -0.07545447 -0.07545447  0.19234496  0.01634998\n",
            "  -0.07144488  0.19234496  0.12786637 -0.00581219  0.12786637  0.01634998\n",
            "  -0.00315784  0.01634998  0.01634998 -0.00372362  0.28854125 -0.04537783\n",
            "   0.12786637  0.01634998 -0.07144488 -0.00581219  0.01634998 -0.00372362\n",
            "  -0.01941636 -0.00372362  0.01901062  0.14927926  0.14927926  0.14927926\n",
            "   0.12786637  0.12786637  0.12786637 -0.10205688 -0.00315784 -0.07144488\n",
            "   0.01066443 -0.00372362 -0.00315784 -0.00372362 -0.04537783 -0.07545447\n",
            "  -0.07144488  0.14927926  0.19234496  0.19234496  0.14927926  0.14927926\n",
            "  -0.00372362  0.14927926 -0.04537783 -0.00372362  0.12786637 -0.07545447\n",
            "   0.01634998  0.14927926 -0.00372362  0.19234496  0.01634998 -0.14120243\n",
            "  -0.07144488 -0.04537783  0.38468991  0.12786637 -0.07144488 -0.00315784\n",
            "  -0.09587143 -0.00315784 -0.10205688 -0.00315784 -0.07144488  0.01634998]]\n",
            "Document embeddings:\n",
            "Document 0: [-0.65503505  0.08756422 -0.23535281]\n",
            "Document 1: [-0.59749834  0.37524266 -0.13820511]\n",
            "Document 2: [-0.42550279  0.30898413 -0.22702776]\n",
            "Document 3: [-0.26376106  0.21932515  0.50295872]\n",
            "Document 4: [-0.07958578  0.07452861  0.70868065]\n",
            "Document 5: [-0.3018436  -0.08110509  0.45279548]\n",
            "Document 6: [-0.3235538  -0.70737842 -0.01379984]\n",
            "Document 7: [-0.39291898 -0.5706551  -0.01102955]\n",
            "Document 8: [-0.28178756  0.00269099  0.05822144]\n",
            "\n",
            "Word embeddings:\n",
            "algorithms: [-0.01598763 -0.06461839 -0.08326329 -0.21075079 -0.06678949 -0.06308913\n",
            " -0.08326329 -0.14520269 -0.15543487 -0.15543487 -0.01598763 -0.05856991\n",
            " -0.09910895 -0.01598763 -0.06308913 -0.12490322 -0.06308913 -0.05856991\n",
            " -0.08326329 -0.05856991 -0.05856991 -0.06461839 -0.06244244 -0.14520269\n",
            " -0.06308913 -0.05856991 -0.09910895 -0.12490322 -0.05856991 -0.06461839\n",
            " -0.33859892 -0.06461839 -0.25014487 -0.05794236 -0.05794236 -0.05794236\n",
            " -0.06308913 -0.06308913 -0.06308913 -0.25392327 -0.08326329 -0.09910895\n",
            " -0.1040468  -0.06461839 -0.08326329 -0.06461839 -0.14520269 -0.15543487\n",
            " -0.09910895 -0.05794236 -0.01598763 -0.01598763 -0.05794236 -0.05794236\n",
            " -0.06461839 -0.05794236 -0.14520269 -0.06461839 -0.06308913 -0.15543487\n",
            " -0.05856991 -0.05794236 -0.06461839 -0.01598763 -0.05856991 -0.29356151\n",
            " -0.09910895 -0.14520269 -0.03197526 -0.06308913 -0.09910895 -0.08326329\n",
            " -0.22994107 -0.08326329 -0.25392327 -0.08326329 -0.09910895 -0.05856991]\n",
            "amount: [ 0.01950736 -0.18407223 -0.15756187  0.10396918 -0.00217927 -0.02208755\n",
            " -0.15756187  0.11881656  0.02707307  0.02707307  0.01950736  0.00072877\n",
            "  0.09377207  0.01950736 -0.02208755 -0.2885496  -0.02208755  0.00072877\n",
            " -0.15756187  0.00072877  0.00072877 -0.18407223  0.0694987   0.11881656\n",
            " -0.02208755  0.00072877  0.09377207 -0.2885496   0.00072877 -0.18407223\n",
            " -0.20086966 -0.18407223  0.0764376   0.06277703  0.06277703  0.06277703\n",
            " -0.02208755 -0.02208755 -0.02208755  0.1232207  -0.15756187  0.09377207\n",
            " -0.1548548  -0.18407223 -0.15756187 -0.18407223  0.11881656  0.02707307\n",
            "  0.09377207  0.06277703  0.01950736  0.01950736  0.06277703  0.06277703\n",
            " -0.18407223  0.06277703  0.11881656 -0.18407223 -0.02208755  0.02707307\n",
            "  0.00072877  0.06277703 -0.18407223  0.01950736  0.00072877  0.17600016\n",
            "  0.09377207  0.11881656  0.03901471 -0.02208755  0.09377207 -0.15756187\n",
            "  0.08928013 -0.15756187  0.1232207  -0.15756187  0.09377207  0.00072877]\n",
            "an: [ 0.19234496 -0.00372362 -0.00315784  0.1609925   0.27045558  0.12786637\n",
            " -0.00315784 -0.04537783 -0.07545447 -0.07545447  0.19234496  0.01634998\n",
            " -0.07144488  0.19234496  0.12786637 -0.00581219  0.12786637  0.01634998\n",
            " -0.00315784  0.01634998  0.01634998 -0.00372362  0.28854125 -0.04537783\n",
            "  0.12786637  0.01634998 -0.07144488 -0.00581219  0.01634998 -0.00372362\n",
            " -0.01941636 -0.00372362  0.01901062  0.14927926  0.14927926  0.14927926\n",
            "  0.12786637  0.12786637  0.12786637 -0.10205688 -0.00315784 -0.07144488\n",
            "  0.01066443 -0.00372362 -0.00315784 -0.00372362 -0.04537783 -0.07545447\n",
            " -0.07144488  0.14927926  0.19234496  0.19234496  0.14927926  0.14927926\n",
            " -0.00372362  0.14927926 -0.04537783 -0.00372362  0.12786637 -0.07545447\n",
            "  0.01634998  0.14927926 -0.00372362  0.19234496  0.01634998 -0.14120243\n",
            " -0.07144488 -0.04537783  0.38468991  0.12786637 -0.07144488 -0.00315784\n",
            " -0.09587143 -0.00315784 -0.10205688 -0.00315784 -0.07144488  0.01634998]\n"
          ]
        }
      ],
      "source": [
        "documents = [\n",
        "    # SpongeBob SquarePants\n",
        "    \"SpongeBob lives in a pineapple under the sea in Bikini Bottom.\",\n",
        "    \"Patrick Star is SpongeBob's best friend and lives under a rock.\",\n",
        "    \"Squidward Tentacles is SpongeBob's grumpy neighbor who plays the clarinet.\",\n",
        "\n",
        "    # Computer Science\n",
        "    \"Python is a popular programming language known for its simplicity and readability.\",\n",
        "    \"Algorithms are step-by-step procedures for solving computational problems.\",\n",
        "    \"Data structures like arrays and linked lists are fundamental in computer science.\",\n",
        "\n",
        "    # Cooking ðŸ”¥ðŸ”¥ðŸ”¥\n",
        "    \"SautÃ©ing involves cooking food quickly in a small amount of oil over high heat.\",\n",
        "    \"Baking is a cooking method that uses dry heat, typically in an oven.\",\n",
        "    \"Seasoning with herbs and spices can greatly enhance the flavor of dishes.\"\n",
        "]\n",
        "\n",
        "doc_topic_matrix, term_topic_matrix, terms = lsa(documents, 3)\n",
        "\n",
        "print(doc_topic_matrix)\n",
        "print(term_topic_matrix)\n",
        "\n",
        "document_embeddings = doc_topic_matrix\n",
        "print(\"Document embeddings:\")\n",
        "for i, embedding in enumerate(document_embeddings):\n",
        "    print(f\"Document {i}: {embedding}\")\n",
        "\n",
        "word_embeddings = term_topic_matrix\n",
        "word_embedding_dict = dict(zip(terms, word_embeddings))\n",
        "print(\"\\nWord embeddings:\")\n",
        "for word, embedding in word_embedding_dict.items():\n",
        "    print(f\"{word}: {embedding}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZXispOBwDaN"
      },
      "source": [
        "## Test\n",
        "\n",
        "Jalankan test dibawah ini untuk mengetahui apakah implementasi kalian sudah benar atau belum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "gYsKO96GwITs",
        "outputId": "817473ef-4a68-487d-a26a-b837b5af8e01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[95mTesting SVD implementation...\u001b[0m\n",
            "\u001b[1m\n",
            "Test 1: Comparison with scipy's SVD\u001b[0m\n",
            "Max difference in U: 0.000000\n",
            "Max difference in S: 0.000000\n",
            "Max difference in V: 0.000000\n",
            "\u001b[92mTest 1 Passed!\u001b[0m\n",
            "\u001b[1m\n",
            "Test 2: Reconstruction Error\u001b[0m\n",
            "Reconstruction error: 0.000000\n",
            "\u001b[92mTest 2 Passed!\u001b[0m\n",
            "\u001b[1m\n",
            "Test 3: Small, known matrix\u001b[0m\n",
            "\n",
            "Max difference in U: 0.000000\n",
            "Max difference in S: 0.000000\n",
            "Max difference in V: 0.000000\n",
            "\u001b[92mTest 3 Passed: Small matrix SVD matches scipy results within tolerance\u001b[0m\n",
            "\u001b[1m\n",
            "Test 4: Orthogonality of U and V\u001b[0m\n",
            "Max deviation from identity for U: 0.000000\n",
            "Max deviation from identity for V: 0.000000\n",
            "\u001b[92mTest 4 Passed!\u001b[0m\n",
            "\u001b[1m\n",
            "Test 5: Properties of singular values\u001b[0m\n",
            "Singular values:\n",
            "[2.19869355 0.7975953  0.67087078 0.26096257]\n",
            "\u001b[92mTest 5 Passed!\u001b[0m\n",
            "\u001b[95m\n",
            "Testing LSA implementation...\u001b[0m\n",
            "\u001b[1m\n",
            "Our LSA document-topic matrix:\u001b[0m\n",
            "[[-0.65503505  0.08756422 -0.23535281]\n",
            " [-0.59749834  0.37524266 -0.13820511]\n",
            " [-0.42550279  0.30898413 -0.22702776]\n",
            " [-0.26376106  0.21932515  0.50295872]\n",
            " [-0.07958578  0.07452861  0.70868065]\n",
            " [-0.3018436  -0.08110509  0.45279548]\n",
            " [-0.3235538  -0.70737842 -0.01379984]\n",
            " [-0.39291898 -0.5706551  -0.01102955]\n",
            " [-0.28178756  0.00269099  0.05822144]]\n",
            "\u001b[1m\n",
            "sklearn's LSA document-topic matrix:\u001b[0m\n",
            "[[ 0.65503505 -0.08756422 -0.23535281]\n",
            " [ 0.59749834 -0.37524266 -0.13820511]\n",
            " [ 0.42550279 -0.30898413 -0.22702776]\n",
            " [ 0.26376106 -0.21932515  0.50295872]\n",
            " [ 0.07958578 -0.07452861  0.70868065]\n",
            " [ 0.3018436   0.08110509  0.45279548]\n",
            " [ 0.3235538   0.70737842 -0.01379984]\n",
            " [ 0.39291898  0.5706551  -0.01102955]\n",
            " [ 0.28178756 -0.00269099  0.05822144]]\n",
            "\u001b[92m\n",
            "Shape match: True\u001b[0m\n",
            "\u001b[92m\n",
            "Max relative difference in document distances: 0.000000\u001b[0m\n",
            "\u001b[1m\n",
            "Top terms for each topic (our implementation):\u001b[0m\n",
            "\n",
            "Topic 1:\n",
            "in: -0.3386\n",
            "spongebob: -0.2936\n",
            "lives: -0.2539\n",
            "under: -0.2539\n",
            "is: -0.2501\n",
            "\n",
            "Topic 2:\n",
            "cooking: -0.2885\n",
            "heat: -0.2885\n",
            "in: -0.2009\n",
            "amount: -0.1841\n",
            "food: -0.1841\n",
            "\n",
            "Topic 3:\n",
            "step: 0.3847\n",
            "for: 0.2885\n",
            "are: 0.2705\n",
            "algorithms: 0.1923\n",
            "by: 0.1923\n",
            "\u001b[93m\n",
            "LSA Test: Automated checks completed. Please verify if the top terms match the expected topics:\u001b[0m\n",
            "- SpongeBob\n",
            "- Computer Science\n",
            "- Cooking\n",
            "Note: The order of topics may not match the expected order.\n"
          ]
        }
      ],
      "source": [
        "#@title Algoritma Test\n",
        "\n",
        "from scipy.linalg import svd as scipy_svd\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "class Colors:\n",
        "    HEADER = '\\033[95m'\n",
        "    OKBLUE = '\\033[94m'\n",
        "    OKGREEN = '\\033[92m'\n",
        "    WARNING = '\\033[93m'\n",
        "    FAIL = '\\033[91m'\n",
        "    ENDC = '\\033[0m'\n",
        "    BOLD = '\\033[1m'\n",
        "\n",
        "def print_color(text, color):\n",
        "    print(f\"{color}{text}{Colors.ENDC}\")\n",
        "\n",
        "def test_svd():\n",
        "    print_color(\"Testing SVD implementation...\", Colors.HEADER)\n",
        "\n",
        "    A = np.random.rand(5, 4)\n",
        "    U, S, V = svd(A)\n",
        "    U_scipy, S_scipy, V_scipy = scipy_svd(A, full_matrices=False)\n",
        "\n",
        "    print_color(\"\\nTest 1: Comparison with scipy's SVD\", Colors.BOLD)\n",
        "\n",
        "    max_diff_U = np.max(np.abs(np.abs(U) - np.abs(U_scipy)))\n",
        "    max_diff_S = np.max(np.abs(np.diag(S) - S_scipy))\n",
        "    max_diff_V = np.max(np.abs(np.abs(V) - np.abs(V_scipy)))\n",
        "\n",
        "    print(f\"Max difference in U: {max_diff_U:.6f}\")\n",
        "    print(f\"Max difference in S: {max_diff_S:.6f}\")\n",
        "    print(f\"Max difference in V: {max_diff_V:.6f}\")\n",
        "\n",
        "    tolerance = 1e-4\n",
        "    if max(max_diff_U, max_diff_S, max_diff_V) < tolerance:\n",
        "        print_color(\"Test 1 Passed!\", Colors.OKGREEN)\n",
        "    else:\n",
        "        print_color(\"Test 1 Failed: Large differences detected\", Colors.FAIL)\n",
        "\n",
        "    A_reconstructed = np.dot(U, np.dot(S, V))\n",
        "    reconstruction_error = np.linalg.norm(A - A_reconstructed)\n",
        "    print_color(\"\\nTest 2: Reconstruction Error\", Colors.BOLD)\n",
        "    print(f\"Reconstruction error: {reconstruction_error:.6f}\")\n",
        "\n",
        "    if reconstruction_error < 1e-5:\n",
        "        print_color(\"Test 2 Passed!\", Colors.OKGREEN)\n",
        "    else:\n",
        "        print_color(\"Test 2 Failed: Large reconstruction error\", Colors.FAIL)\n",
        "\n",
        "    print_color(\"\\nTest 3: Small, known matrix\", Colors.BOLD)\n",
        "\n",
        "    A_small = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "    U_small, S_small, V_small = svd(A_small)\n",
        "\n",
        "    U_scipy, S_scipy, V_scipy = scipy_svd(A_small, full_matrices=False)\n",
        "\n",
        "    max_diff_U = np.max(np.abs(np.abs(U_small) - np.abs(U_scipy)))\n",
        "    max_diff_S = np.max(np.abs(np.diag(S_small) - S_scipy))\n",
        "    max_diff_V = np.max(np.abs(np.abs(V_small) - np.abs(V_scipy.T)))\n",
        "\n",
        "    tolerance = 1e-6\n",
        "\n",
        "    print(f\"\\nMax difference in U: {max_diff_U:.6f}\")\n",
        "    print(f\"Max difference in S: {max_diff_S:.6f}\")\n",
        "    print(f\"Max difference in V: {max_diff_V:.6f}\")\n",
        "\n",
        "    if max(max_diff_U, max_diff_S, max_diff_V) < tolerance:\n",
        "        print_color(\"Test 3 Passed: Small matrix SVD matches scipy results within tolerance\", Colors.OKGREEN)\n",
        "    else:\n",
        "        print_color(\"Test 3 Failed: Large differences detected in small matrix SVD\", Colors.FAIL)\n",
        "\n",
        "    U_orthogonality = np.dot(U.T, U)\n",
        "    V_orthogonality = np.dot(V, V.T)\n",
        "    print_color(\"\\nTest 4: Orthogonality of U and V\", Colors.BOLD)\n",
        "\n",
        "    U_ortho_error = np.max(np.abs(U_orthogonality - np.eye(U_orthogonality.shape[0])))\n",
        "    V_ortho_error = np.max(np.abs(V_orthogonality - np.eye(V_orthogonality.shape[0])))\n",
        "\n",
        "    print(f\"Max deviation from identity for U: {U_ortho_error:.6f}\")\n",
        "    print(f\"Max deviation from identity for V: {V_ortho_error:.6f}\")\n",
        "\n",
        "    if max(U_ortho_error, V_ortho_error) < 1e-6:\n",
        "        print_color(\"Test 4 Passed!\", Colors.OKGREEN)\n",
        "    else:\n",
        "        print_color(\"Test 4 Failed: U or V not orthogonal\", Colors.FAIL)\n",
        "\n",
        "    print_color(\"\\nTest 5: Properties of singular values\", Colors.BOLD)\n",
        "    singular_values = np.diag(S)\n",
        "    print(\"Singular values:\")\n",
        "    print(singular_values)\n",
        "\n",
        "    if np.all(singular_values >= 0) and np.all(np.diff(singular_values) <= 0):\n",
        "        print_color(\"Test 5 Passed!\", Colors.OKGREEN)\n",
        "    else:\n",
        "        print_color(\"Test 5 Failed: Singular values not non-negative or not in descending order\", Colors.FAIL)\n",
        "\n",
        "def test_lsa():\n",
        "    print_color(\"\\nTesting LSA implementation...\", Colors.HEADER)\n",
        "\n",
        "    documents = [\n",
        "        # SpongeBob SquarePants\n",
        "        \"SpongeBob lives in a pineapple under the sea in Bikini Bottom.\",\n",
        "        \"Patrick Star is SpongeBob's best friend and lives under a rock.\",\n",
        "        \"Squidward Tentacles is SpongeBob's grumpy neighbor who plays the clarinet.\",\n",
        "\n",
        "        # Computer Science\n",
        "        \"Python is a popular programming language known for its simplicity and readability.\",\n",
        "        \"Algorithms are step-by-step procedures for solving computational problems.\",\n",
        "        \"Data structures like arrays and linked lists are fundamental in computer science.\",\n",
        "\n",
        "        # Cooking\n",
        "        \"SautÃ©ing involves cooking food quickly in a small amount of oil over high heat.\",\n",
        "        \"Baking is a cooking method that uses dry heat, typically in an oven.\",\n",
        "        \"Seasoning with herbs and spices can greatly enhance the flavor of dishes.\"\n",
        "    ]\n",
        "\n",
        "    num_topics = 3\n",
        "\n",
        "    doc_topic_matrix, term_topic_matrix, terms = lsa(documents, num_topics)\n",
        "\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "    lsa_sklearn = TruncatedSVD(n_components=num_topics, random_state=42)\n",
        "    doc_topic_matrix_sklearn = lsa_sklearn.fit_transform(tfidf_matrix)\n",
        "\n",
        "    print_color(\"\\nOur LSA document-topic matrix:\", Colors.BOLD)\n",
        "    print(doc_topic_matrix)\n",
        "    print_color(\"\\nsklearn's LSA document-topic matrix:\", Colors.BOLD)\n",
        "    print(doc_topic_matrix_sklearn)\n",
        "\n",
        "    shape_match = doc_topic_matrix.shape == doc_topic_matrix_sklearn.shape\n",
        "    print_color(f\"\\nShape match: {shape_match}\", Colors.OKGREEN if shape_match else Colors.FAIL)\n",
        "\n",
        "    our_distances = np.linalg.norm(doc_topic_matrix[0] - doc_topic_matrix[1:], axis=1)\n",
        "    sklearn_distances = np.linalg.norm(doc_topic_matrix_sklearn[0] - doc_topic_matrix_sklearn[1:], axis=1)\n",
        "    relative_diff = np.abs(our_distances / our_distances.sum() - sklearn_distances / sklearn_distances.sum())\n",
        "\n",
        "    print_color(f\"\\nMax relative difference in document distances: {np.max(relative_diff):.6f}\",\n",
        "                Colors.OKGREEN if np.max(relative_diff) < 0.1 else Colors.FAIL)\n",
        "\n",
        "    print_color(\"\\nTop terms for each topic (our implementation):\", Colors.BOLD)\n",
        "    for topic in range(num_topics):\n",
        "        top_terms = sorted([(terms[i], term_topic_matrix[topic, i]) for i in range(len(terms))],\n",
        "                           key=lambda x: abs(x[1]), reverse=True)[:5]\n",
        "        print(f\"\\nTopic {topic + 1}:\")\n",
        "        for term, weight in top_terms:\n",
        "            print(f\"{term}: {weight:.4f}\")\n",
        "\n",
        "    print_color(\"\\nLSA Test: Automated checks completed. Please verify if the top terms match the expected topics:\", Colors.WARNING)\n",
        "    print(\"- SpongeBob\")\n",
        "    print(\"- Computer Science\")\n",
        "    print(\"- Cooking\")\n",
        "    print(\"Note: The order of topics may not match the expected order.\")\n",
        "\n",
        "test_svd()\n",
        "test_lsa()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-HzijnFyvAs"
      },
      "source": [
        "# Chapter 2: Neural Embeddings with Word2Vec - CBOW"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pengantar Continuous Bag of Words (CBOW)"
      ],
      "metadata": {
        "id": "8BYYAngq9yi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuous Bag of Words (CBOW) adalah teknik mendapatkan embedding berbasis neural network. CBOW adalah salah satu dari dua arsitektur model yang diperkenalkan dalam framework Word2Vec, bersama dengan Skip-gram.\n",
        "\n",
        "Yang dilakukan CBOW adalah memprediksi kata target berdasarkan kata-kata konteks di sekitarnya. Sebagai contoh, pada kalimat dibawah inki\n",
        "\n",
        "---\n",
        "\n",
        "â€œð™ð™žð™™ð™–ð™  ð™–ð™™ð™– ð™®ð™–ð™£ð™œ ð™¥ð™šð™™ð™ªð™¡ð™ž ð™™ð™šð™£ð™œð™–ð™£ ð™£ð™–ð™¨ð™žð™— ð™—ð™ªð™§ð™ªð™ ð™¨ð™šð™¡ð™–ð™¢ð™– ð™¢ð™šð™§ð™šð™ ð™– ð™—ð™žð™¨ð™– ð™¢ð™šð™£ð™™ð™–ð™¥ð™–ð™©ð™ ð™–ð™£ ð™ ð™šð™¥ð™ªð™–ð™¨ð™–ð™£ ð™¨ð™šð™˜ð™–ð™§ð™– ð™žð™£ð™¨ð™©ð™–ð™£.â€\n",
        "\n",
        "-Squidward Q. Tentacles\n",
        "\n",
        "---\n",
        "\n",
        "arsitektur CBOW dapat mencoba memprediksi kata \"buruh\" berdasarkan kata konteks \"dengan nasib\" dan \"selama mereka.\"\n",
        "\n",
        "Metode ini memungkinkan kita untuk menangkap hubungan semantik antara kata-kata dengan merepresentasikannya sebagai vektor padat dalam ruang vektor. Dalam contoh kita, CBOW akan belajar mengasosiasikan â€œ buruh â€ dengan konsep-konsep yang berkaitan.\n",
        "\n",
        "Dalam tutorial ini, kita akan mengeksplorasi konsep-konsep dasar di balik CBOW, termasuk arsitektur dan proses pelatihannya. Kita akan mengimplementasikan CBOW dari awal menggunakan Python, mendapatkan pemahaman yang mendalam tentang bagaimana model ini mempelajari representasi kata dari kalimat-kalimat seperti contoh yang akan kita pelajari."
      ],
      "metadata": {
        "id": "-JmU8xYV92yB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "mGxoHopoy1LQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Persiapan dataset\n",
        "\n",
        "Disini kita akan mempersiapkan data yang akan kita latih untuk mendapatkan embedding dengan model CBOW. Data yang digunakan adalah sintetis agar mudah untuk melakukan pengecekan. Kamu cukup jalankan saja dibawah ini."
      ],
      "metadata": {
        "id": "bEaBO3fwMaMP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "69xG0OOdsF9b",
        "outputId": "fc8b636a-28a5-425c-9db6-dc14d654f412",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmented vocabulary size: 147\n"
          ]
        }
      ],
      "source": [
        "def augment_dataset(sentences = [], num_augmented=200):\n",
        "    templates = [\n",
        "        \"{character} {action} in {location}\",\n",
        "        \"{character} and {character} {action} at the {location}\",\n",
        "        \"{action} is important for {character} at the {location}\",\n",
        "        \"{character} {action} with {character} near {location}\",\n",
        "        \"oh barnacles {character} {action} again at the {location}\",\n",
        "        \"who lives in a {location} under the sea {character}\",\n",
        "        \"{character} loves to {action} more than anything else in {location}\",\n",
        "        \"tartar sauce {character} forgot to {action} at the {location}\",\n",
        "        \"{character} is ready to {action} at {location}\",\n",
        "        \"im ready im ready says {character} before they {action} at {location}\",\n",
        "        \"{character} and {character} have a {adjective} time when they {action}\",\n",
        "        \"the best time to {action} is all the time says {character}\",\n",
        "        \"{character} learns that {action} is not as easy as it looks\",\n",
        "        \"money money money chants {character} while they {action}\",\n",
        "        \"{character} tries to {action} but ends up making a {adjective} mess\",\n",
        "        \"sweet victory {character} finally manages to {action} successfully\",\n",
        "        \"{character} and {character} compete to see who can {action} better\",\n",
        "        \"imagination {character} uses it to {action} in a {adjective} way\",\n",
        "        \"{character} discovers a new way to {action} at {location}\",\n",
        "        \"oh {exclamation} {character} cant believe they have to {action} again\"\n",
        "    ]\n",
        "\n",
        "    characters = ['spongebob', 'patrick', 'squidward', 'mr krabs', 'sandy', 'plankton', 'gary', 'mrs puff', 'pearl', 'larry the lobster', 'mermaid man', 'barnacle boy', 'flying dutchman']\n",
        "    actions = ['laughs', 'works', 'sings', 'dances', 'cooks', 'sleeps', 'plays', 'swims', 'jellyfishes', 'karates', 'bubbles', 'plans', 'dreams', 'practices', 'learns', 'teaches', 'explores', 'invents', 'relaxes', 'exercises']\n",
        "    locations = ['krusty krab', 'bikini bottom', 'pineapple house', 'chum bucket', 'goo lagoon', 'jellyfish fields', 'boating school', 'barg n mart', 'weenie hut juniors', 'salty spitoon', 'treedome', 'rock bottom', 'kelp forest']\n",
        "    adjectives = ['fun', 'crazy', 'silly', 'exciting', 'dangerous', 'hilarious', 'weird', 'amazing', 'unexpected', 'fantastic']\n",
        "    exclamations = ['barnacles', 'fishpaste', 'tartar sauce', 'holy shrimp', 'great barrier reef']\n",
        "\n",
        "    augmented_sentences = sentences.copy()\n",
        "\n",
        "    for _ in range(num_augmented):\n",
        "        template = random.choice(templates)\n",
        "        new_sentence = template.format(\n",
        "            character=random.choice(characters),\n",
        "            action=random.choice(actions),\n",
        "            location=random.choice(locations),\n",
        "            adjective=random.choice(adjectives),\n",
        "            exclamation=random.choice(exclamations)\n",
        "        )\n",
        "        augmented_sentences.append(new_sentence)\n",
        "\n",
        "    return augmented_sentences\n",
        "\n",
        "augmented_sentences = augment_dataset()\n",
        "\n",
        "tokenized_sentences = [sentence.lower().split() for sentence in augmented_sentences]\n",
        "\n",
        "vocab = list(set(word for sentence in tokenized_sentences for word in sentence))\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "\n",
        "print(f\"Augmented vocabulary size: {vocab_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## input-output pairs\n",
        "\n",
        "Karena dalam CBOW kita akan memprediksi menggunakan konteks sekitar dari suatu kata, oleh karena itu disini kita akan mengimplementasikan bagaimana cara membuat data yang akan ditrain.\n",
        "\n",
        "Feel free untuk mengikuti arahan dibawah atau mencoba cara sendiri.\n",
        "\n",
        "1. Deklarasikan fungsi dan struktur data penyimpan\n",
        "2. Lakukan iterasi terhadap documents dan words\n",
        "3. Buat representasi konteks:\n",
        "   - Inisialisasi vektor nol dengan panjang yang sama dengan ukuran vocab.\n",
        "   - Vektor ini akan merepresentasikan kata-kata konteks sebagai bag of words.\n",
        "4. Isi vektor konteks dengan vektor yang berada di window.\n",
        "5. Ambil indeks kata target, ubah kata target menjadi indeksnya di dalam kosakata. Ini yang akan menjadi label yang kita coba prediksi dalam model CBOW.\n",
        "6. Buat dan simpan pairs\n",
        "7. Return pairs\n"
      ],
      "metadata": {
        "id": "0-LPq65ZMmQo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "O9kv64v1slVC",
        "outputId": "de5d9ad9-e765-4a7f-8ac3-149a3ad5a569",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1609"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "def create_cbow_pairs(tokenized_documents, window_size=2):\n",
        "    # Assumtion: Window Size kelipatan 2\n",
        "    pairs = []\n",
        "    for doc in tokenized_documents:\n",
        "        for i, word in enumerate(doc):\n",
        "          context_vector = torch.zeros(vocab_size)\n",
        "\n",
        "          prev_context = [word_to_idx[word] for word in doc[(i - (window_size)//2) : i]]\n",
        "          next_context = [word_to_idx[word] for word in doc[i + 1 : (i + (window_size)//2 + 1)]]\n",
        "          contexts = prev_context + next_context\n",
        "          for context in contexts:\n",
        "            context_vector[context] += 1\n",
        "\n",
        "          if (len(prev_context) + len(next_context)) == window_size:\n",
        "              pairs.append((context_vector, word_to_idx[word]))\n",
        "          else:\n",
        "            continue\n",
        "\n",
        "    return pairs\n",
        "\n",
        "cbow_pairs = create_cbow_pairs(tokenized_sentences)\n",
        "len(cbow_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Persiapan Model\n",
        "\n",
        "Disini kita akan mempersiapkan matriks yang akan kita train dengan data yang sudah kita olah barusan, dimana:\n",
        "\n",
        "W1 (Input ke hidden layer):\n",
        "- W1 adalah matriks weight antara input layer dan lapisan hidden layer.\n",
        "- Shapenya adalah ( vocabulary_size, embedding_size ).\n",
        "- Setiap baris dalam W1 korespon dengan sebuah kata dalam vocab.\n",
        "- Setelah training, W1 berisi embedding kata yang telah dipelajari.\n",
        "- Embeddings ini yang harapannya akan menangkap makna semantik dari kata-kata.\n",
        "\n",
        "W2 (Hidden layer ke output):\n",
        "- W2 adalah matriks weight antara hidden layer dan output layer.\n",
        "- Shapenya adalah (embedding_size, vocabulary_size).\n",
        "- Matriks ini membantu dalam memprediksi distribusi probabilitas atas semua kata dalam vocab.\n",
        "\n",
        "Masing-masing silakan inisialisasi dengan matriks random."
      ],
      "metadata": {
        "id": "6eUdHhkDP7tw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "1PXffCK9snyl"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 50\n",
        "\n",
        "W1 = torch.randn(vocab_size, embedding_dim, requires_grad=True)\n",
        "W2 = torch.randn(embedding_dim, vocab_size, requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "Setelah menyiapkan data, kita perlu melatih model CBOW. Hal ini berarti kita harus implementasi penerusan jaringan saraf dan training loop secara manual."
      ],
      "metadata": {
        "id": "EyKzF7-7QGuw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward Pass:\n",
        "   - Fungsi ini merepresentasikan arsitektur NN untuk CBOW.\n",
        "   - Langkah-langkah dalam proses forward:\n",
        "     \n",
        "     - Kalikan matriks konteks dengan W1 untuk mendapatkan representasi hidden layer.\n",
        "     \n",
        "     - Kalikan kembali hidden layer dengan W2 untuk mendapatkan output score untuk setiap kata dalam vocab.\n",
        "   - Kembalikan output."
      ],
      "metadata": {
        "id": "5kzFa_nLQ3kG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(context, W1, W2):\n",
        "  # TODO\n",
        "\n",
        "  hidden_layer = context @ W1\n",
        "\n",
        "  output = hidden_layer @ W2\n",
        "\n",
        "  return output"
      ],
      "metadata": {
        "id": "xDZBZWdjACBi"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop\n",
        "Setelah mengimplementasikan function forward, kita dapat memulai membuat training loopnya. Pertama kita juga harus siapkan loss function, untuk program ini kita dapat gunakan CrossEntropyLoss. Selanjutnya kita buat loop untuk setiap epoch, yang dimana setiap epoch kita ingin update weights dari semua data training, sehingg setiap loop untuk pasangan konteks dan target dapat kita lakukan:\n",
        "\n",
        "1. Panggil fungsi `forward` dengan matriks konteks dan bobot saat ini.\n",
        "2. Hitung loss\n",
        "3. Lakukan backward pass\n",
        "4. Update weight\n",
        "   - Gunakan gradien dan learning rate untuk memperbarui W1 dan W2.\n",
        "5. Reset kembali gradien\n",
        "6. Epoch selanjutnya\n",
        "\n",
        "kemudian return kembali W1 dan W2"
      ],
      "metadata": {
        "id": "8Rmp-FzURvfc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "GPEWXKe-s9Jf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "045e038d-a152-4dde-eb2d-5b4a051f46e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-68-531617fa494d>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  context_tensor = torch.tensor(context, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 17.244158229350955\n",
            "Epoch 2, Loss: 7.1060341247561505\n",
            "Epoch 3, Loss: 3.9551063357717084\n",
            "Epoch 4, Loss: 2.6184376582920295\n",
            "Epoch 5, Loss: 1.8932258064123466\n",
            "Epoch 6, Loss: 1.4756565577602552\n",
            "Epoch 7, Loss: 1.2076955248965695\n",
            "Epoch 8, Loss: 1.02352085748933\n",
            "Epoch 9, Loss: 0.8910857202432694\n",
            "Epoch 10, Loss: 0.7939291941787796\n",
            "Epoch 11, Loss: 0.723417457510284\n",
            "Epoch 12, Loss: 0.6720385171341876\n",
            "Epoch 13, Loss: 0.6329504062033922\n",
            "Epoch 14, Loss: 0.6023338082290444\n",
            "Epoch 15, Loss: 0.5778357480100932\n",
            "Epoch 16, Loss: 0.5578441980205562\n",
            "Epoch 17, Loss: 0.5414784614244802\n",
            "Epoch 18, Loss: 0.5281954567173242\n",
            "Epoch 19, Loss: 0.5175235916300022\n",
            "Epoch 20, Loss: 0.5089790097949802\n",
            "Epoch 21, Loss: 0.5021180593275874\n",
            "Epoch 22, Loss: 0.49660416416217423\n",
            "Epoch 23, Loss: 0.4922533168428554\n",
            "Epoch 24, Loss: 0.48885032179879934\n",
            "Epoch 25, Loss: 0.4860692167227033\n",
            "Epoch 26, Loss: 0.4837020929057242\n",
            "Epoch 27, Loss: 0.4816397378575361\n",
            "Epoch 28, Loss: 0.47981691211696303\n",
            "Epoch 29, Loss: 0.478190364887653\n",
            "Epoch 30, Loss: 0.47672910343805475\n",
            "Epoch 31, Loss: 0.47540820594033123\n",
            "Epoch 32, Loss: 0.4742062779132287\n",
            "Epoch 33, Loss: 0.4731047264734469\n",
            "Epoch 34, Loss: 0.4720876629161763\n",
            "Epoch 35, Loss: 0.47114191826053087\n",
            "Epoch 36, Loss: 0.47025683973740817\n",
            "Epoch 37, Loss: 0.46942398290544374\n",
            "Epoch 38, Loss: 0.46863639185853534\n",
            "Epoch 39, Loss: 0.4678885620223219\n",
            "Epoch 40, Loss: 0.4671760620826794\n",
            "Epoch 41, Loss: 0.46649459625603995\n",
            "Epoch 42, Loss: 0.46584131519793043\n",
            "Epoch 43, Loss: 0.46521328570751225\n",
            "Epoch 44, Loss: 0.46460812322781775\n",
            "Epoch 45, Loss: 0.4640237635539824\n",
            "Epoch 46, Loss: 0.4634584556876585\n",
            "Epoch 47, Loss: 0.46291064050041686\n",
            "Epoch 48, Loss: 0.4623789995448003\n",
            "Epoch 49, Loss: 0.4618620543667139\n",
            "Epoch 50, Loss: 0.46135906946533073\n",
            "Epoch 51, Loss: 0.4608687216363149\n",
            "Epoch 52, Loss: 0.46039038298766394\n",
            "Epoch 53, Loss: 0.4599232709446473\n",
            "Epoch 54, Loss: 0.4594665887743096\n",
            "Epoch 55, Loss: 0.45901973221887177\n",
            "Epoch 56, Loss: 0.458582125102075\n",
            "Epoch 57, Loss: 0.458153266466952\n",
            "Epoch 58, Loss: 0.457732666898879\n",
            "Epoch 59, Loss: 0.4573198096641164\n",
            "Epoch 60, Loss: 0.4569144781843324\n",
            "Epoch 61, Loss: 0.45651611271083237\n",
            "Epoch 62, Loss: 0.4561245054097428\n",
            "Epoch 63, Loss: 0.4557393552554807\n",
            "Epoch 64, Loss: 0.45536021925363424\n",
            "Epoch 65, Loss: 0.4549869296322299\n",
            "Epoch 66, Loss: 0.4546193094067919\n",
            "Epoch 67, Loss: 0.4542569156777861\n",
            "Epoch 68, Loss: 0.45389973507514203\n",
            "Epoch 69, Loss: 0.4535475334604559\n",
            "Epoch 70, Loss: 0.45320001131567306\n",
            "Epoch 71, Loss: 0.45285710848006433\n",
            "Epoch 72, Loss: 0.45251855995348145\n",
            "Epoch 73, Loss: 0.4521842435182081\n",
            "Epoch 74, Loss: 0.4518540814155751\n",
            "Epoch 75, Loss: 0.45152789691627837\n",
            "Epoch 76, Loss: 0.4512055316298249\n",
            "Epoch 77, Loss: 0.45088686045807236\n",
            "Epoch 78, Loss: 0.4505717818637644\n",
            "Epoch 79, Loss: 0.4502602512988254\n",
            "Epoch 80, Loss: 0.44995201463654483\n",
            "Epoch 81, Loss: 0.44964710384911866\n",
            "Epoch 82, Loss: 0.44934535357863986\n",
            "Epoch 83, Loss: 0.4490466922933464\n",
            "Epoch 84, Loss: 0.44875104271208144\n",
            "Epoch 85, Loss: 0.4484584170590283\n",
            "Epoch 86, Loss: 0.4481685463158353\n",
            "Epoch 87, Loss: 0.4478814579050144\n",
            "Epoch 88, Loss: 0.44759703914122406\n",
            "Epoch 89, Loss: 0.4473153261873687\n",
            "Epoch 90, Loss: 0.44703616567992804\n",
            "Epoch 91, Loss: 0.44675956431162583\n",
            "Epoch 92, Loss: 0.4464854134695468\n",
            "Epoch 93, Loss: 0.44621356509892246\n",
            "Epoch 94, Loss: 0.4459441444693041\n",
            "Epoch 95, Loss: 0.4456770454018032\n",
            "Epoch 96, Loss: 0.4454120764790722\n",
            "Epoch 97, Loss: 0.44514945706572223\n",
            "Epoch 98, Loss: 0.44488886511275333\n",
            "Epoch 99, Loss: 0.4446303739874059\n",
            "Epoch 100, Loss: 0.4443739830646465\n"
          ]
        }
      ],
      "source": [
        "def train_cbow(cbow_pairs, W1, W2, num_epochs=100, learning_rate=0.01):\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for context, target_index in cbow_pairs:\n",
        "            context_tensor = torch.tensor(context, dtype=torch.float32)\n",
        "            target_tensor = torch.tensor(target_index, dtype=torch.long)\n",
        "\n",
        "            # TODO: Forward pass\n",
        "            output = forward(context_tensor, W1, W2)\n",
        "            # TODO: Compute loss\n",
        "            loss = loss_fn(output, target_tensor)\n",
        "\n",
        "            # TODO: Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # TODO: Update weights\n",
        "\n",
        "            with torch.no_grad():\n",
        "              W1 -= learning_rate * W1.grad\n",
        "              W1.grad.zero_()\n",
        "\n",
        "              W2 -= learning_rate * W2.grad\n",
        "              W2.grad.zero_()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(cbow_pairs)}\")\n",
        "\n",
        "    return W1, W2\n",
        "\n",
        "\n",
        "W1, W2 = train_cbow(cbow_pairs, W1, W2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Penggunaan\n",
        "\n",
        "Setelah melatih model CBOW, kita dapat mendapatkan word embedding yang telah dipelajari untuk menemukan kata-kata yang mirip secara semantik.\n",
        "\n",
        "Kita disini bisa detach matriks W1 dari grafik komputasi untuk menggunakannya sebagai embedding.\n",
        "\n",
        "Matriks ini berisi vektor kata yang telah dipelajari untuk setiap kata dalam vocab kita."
      ],
      "metadata": {
        "id": "xhea4L0rUG6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_embeddings = W1.detach()\n",
        "word_embeddings.shape"
      ],
      "metadata": {
        "id": "HtoPu43KzQMS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4f8080d-c6eb-495f-983e-09e161712c02"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([147, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dibawah ini adalah contoh untuk mencari kata yang serupa."
      ],
      "metadata": {
        "id": "5POehKbNUtjH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "uF038t9MtPSV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94046408-ff1a-496c-e449-933242c55692"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words similar to 'krusty': ['swims', 'while', 'fishpaste', 'bikini', 'sings']\n"
          ]
        }
      ],
      "source": [
        "def get_similar_words(word, word_to_idx, idx_to_word, word_embeddings, top_k=5):\n",
        "    if word not in word_to_idx:\n",
        "        return []\n",
        "\n",
        "    word_idx = word_to_idx[word]\n",
        "    word_embedding = word_embeddings[word_idx]\n",
        "\n",
        "    similarities = torch.matmul(word_embeddings, word_embedding)\n",
        "    top_indices = torch.argsort(similarities, descending=True)[1:top_k+1]\n",
        "\n",
        "    return [idx_to_word[idx.item()] for idx in top_indices]\n",
        "\n",
        "\n",
        "\n",
        "word = \"krusty\"\n",
        "similar_words = get_similar_words(word, word_to_idx, idx_to_word, word_embeddings)\n",
        "print(f\"Words similar to '{word}': {similar_words}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "1_QbZ_YF5V95"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}