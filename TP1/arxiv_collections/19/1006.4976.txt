Type Ia supernovae (Ia SNe) provide a rich source of iron for hot gas in
galactic stellar spheroids. However, the expected super-solar iron abundance of
the hot gas is not observed. Instead, X-ray observations often show decreasing
iron abundance toward galactic central regions, where the Ia SN enrichment is
expected to be the highest. We examine the cause of this missing iron problem
by studying the enrichment process and its effect on X-ray abundance
measurements of the hot gas. The evolution of Ia SN iron ejecta is simulated in
the context of galaxy-wide hot gas outflows, in both supersonic and subsonic
cases, as may be expected for hot gas in galactic bulges or elliptical galaxies
of intermediate masses. SN reverse-shock heated iron ejecta is typically found
to have a very high temperature and low density, hence producing little X-ray
emission. Such hot ejecta, driven by its large buoyancy, can quickly reach a
substantially higher outward velocity than the ambient medium, which is
dominated by mass loss from evolved stars. The ejecta is gradually and
dynamically mixed with the medium at large galactic radii. The ejecta is also
slowly diluted and cooled by {\sl insitu} mass injection from evolved stars.
These processes together naturally result in the observed positive gradient in
the average radial iron abundance distribution of the hot gas, even if
mass-weighted. This trend is in addition to the X-ray measurement bias that
tends to underestimate the iron abundance for the hot gas with a temperature
distribution.