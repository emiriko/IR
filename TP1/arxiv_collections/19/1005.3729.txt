$\ell_1$ minimization is often used for finding the sparse solutions of an
under-determined linear system. In this paper we focus on finding sharp
performance bounds on recovering approximately sparse signals using $\ell_1$
minimization, possibly under noisy measurements. While the restricted isometry
property is powerful for the analysis of recovering approximately sparse
signals with noisy measurements, the known bounds on the achievable sparsity
(The "sparsity" in this paper means the size of the set of nonzero or
significant elements in a signal vector.) level can be quite loose. The
neighborly polytope analysis which yields sharp bounds for ideally sparse
signals cannot be readily generalized to approximately sparse signals. Starting
from a necessary and sufficient condition, the "balancedness" property of
linear subspaces, for achieving a certain signal recovery accuracy, we give a
unified \emph{null space Grassmann angle}-based geometric framework for
analyzing the performance of $\ell_1$ minimization. By investigating the
"balancedness" property, this unified framework characterizes sharp
quantitative tradeoffs between the considered sparsity and the recovery
accuracy of the $\ell_{1}$ optimization. As a consequence, this generalizes the
neighborly polytope result for ideally sparse signals. Besides the robustness
in the "strong" sense for \emph{all} sparse signals, we also discuss the
notions of "weak" and "sectional" robustness. Our results concern fundamental
properties of linear subspaces and so may be of independent mathematical
interest.