Despite strong evidence of widespread errors, spreadsheet developers rarely
subject their spreadsheets to post-development testing to reduce errors. This
may be because spreadsheet developers are overconfident in the accuracy of
their spreadsheets. This conjecture is plausible because overconfidence is
present in a wide variety of human cognitive domains, even among experts. This
paper describes two experiments in overconfidence in spreadsheet development.
The first is a pilot study to determine the existence of overconfidence. The
second tests a manipulation to reduce overconfidence and errors. The
manipulation is modestly successful, indicating that overconfidence reduction
is a promising avenue to pursue.