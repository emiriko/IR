The start of data taking at the Large Hadron Collider will herald a new era
in data volumes and distributed processing in particle physics. Data volumes of
hundreds of Terabytes will be shipped to Tier-2 centres for analysis by the LHC
experiments using the Worldwide LHC Computing Grid (WLCG).
  In many countries Tier-2 centres are distributed between a number of
institutes, e.g., the geographically spread Tier-2s of GridPP in the UK. This
presents a number of challenges for experiments to utilise these centres
efficaciously, as CPU and storage resources may be sub-divided and exposed in
smaller units than the experiment would ideally want to work with. In addition,
unhelpful mismatches between storage and CPU at the individual centres may be
seen, which make efficient exploitation of a Tier-2's resources difficult.
  One method of addressing this is to unify the storage across a distributed
Tier-2, presenting the centres' aggregated storage as a single system. This
greatly simplifies data management for the VO, which then can access a greater
amount of data across the Tier-2. However, such an approach will lead to
scenarios where analysis jobs on one site's batch system must access data
hosted on another site.
  We investigate this situation using the Glasgow and Edinburgh clusters, which
are part of the ScotGrid distributed Tier-2. In particular we look at how to
mitigate the problems associated with ``distant'' data access and discuss the
security implications of having LAN access protocols traverse the WAN between
centres.