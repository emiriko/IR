In studying the complexity of iterative processes it is usually assumed that
the arithmetic operations of addition, multiplication, and division can be
performed in certain constant times. This assumption is invalid if the
precision required increases as the computation proceeds. We give upper and
lower bounds on the number of single-precision operations required to perform
various multiple-precision operations, and deduce some interesting consequences
concerning the relative efficiencies of methods for solving nonlinear equations
using variable-length multiple-precision arithmetic. A postscript describes
more recent developments.