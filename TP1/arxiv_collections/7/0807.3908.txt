The Resource Description Framework (RDF) is continuing to grow outside the
bounds of its initial function as a metadata framework and into the domain of
general-purpose data modeling. This expansion has been facilitated by the
continued increase in the capacity and speed of RDF database repositories known
as triple-stores. High-end RDF triple-stores can hold and process on the order
of 10 billion triples. In an effort to provide a seamless integration of the
data contained in RDF repositories, the Linked Data community is providing
specifications for linking RDF data sets into a universal distributed graph
that can be traversed by both man and machine. While the seamless integration
of RDF data sets is important, at the scale of the data sets that currently
exist and will ultimately grow to become, the "download and index" philosophy
of the World Wide Web will not so easily map over to the Semantic Web. This
essay discusses the importance of adding a distributed RDF process
infrastructure to the current distributed RDF data structure.