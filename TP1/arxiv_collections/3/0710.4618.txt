The incorporation of unlabeled data in regression and classification analysis
is an increasing focus of the applied statistics and machine learning
literatures, with a number of recent examples demonstrating the potential for
unlabeled data to contribute to improved predictive accuracy. The statistical
basis for this semisupervised analysis does not appear to have been well
delineated; as a result, the underlying theory and rationale may be
underappreciated, especially by nonstatisticians. There is also room for
statisticians to become more fully engaged in the vigorous research in this
important area of intersection of the statistical and computer sciences. Much
of the theoretical work in the literature has focused, for example, on
geometric and structural properties of the unlabeled data in the context of
particular algorithms, rather than probabilistic and statistical questions.
This paper overviews the fundamental statistical foundations for predictive
modeling and the general questions associated with unlabeled data, highlighting
the relevance of venerable concepts of sampling design and prior specification.
This theory, illustrated with a series of central illustrative examples and two
substantial real data analyses, shows precisely when, why and how unlabeled
data matter.