We characterise the convergence of the Gibbs sampler which samples from the
joint posterior distribution of parameters and missing data in hierarchical
linear models with arbitrary symmetric error distributions. We show that the
convergence can be uniform, geometric or sub-geometric depending on the
relative tail behaviour of the error distributions, and on the parametrisation
chosen. Our theory is applied to characterise the convergence of the Gibbs
sampler on latent Gaussian process models. We indicate how the theoretical
framework we introduce will be useful in analyzing more complex models.