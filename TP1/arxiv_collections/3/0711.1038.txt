In this article, we present an approach for non native automatic speech
recognition (ASR). We propose two methods to adapt existing ASR systems to the
non-native accents. The first method is based on the modification of acoustic
models through integration of acoustic models from the mother tong. The
phonemes of the target language are pronounced in a similar manner to the
native language of speakers. We propose to combine the models of confused
phonemes so that the ASR system could recognize both concurrent
pronounciations. The second method we propose is a refinment of the
pronounciation error detection through the introduction of graphemic
constraints. Indeed, non native speakers may rely on the writing of words in
their uttering. Thus, the pronounctiation errors might depend on the characters
composing the words. The average error rate reduction that we observed is
(22.5%) relative for the sentence error rate, and 34.5% (relative) in word
error rate.