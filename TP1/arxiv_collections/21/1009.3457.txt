Computing on graphics processors is maybe one of the most important
developments in computational science to happen in decades. Not since the
arrival of the Beowulf cluster, which combined open source software with
commodity hardware to truly democratize high-performance computing, has the
community been so electrified. Like then, the opportunity comes with
challenges. The formulation of scientific algorithms to take advantage of the
performance offered by the new architecture requires rethinking core methods.
Here, we have tackled fast summation algorithms (fast multipole method and fast
Gauss transform), and applied algorithmic redesign for attaining performance on
gpus. The progression of performance improvements attained illustrates the
exercise of formulating algorithms for the massively parallel architecture of
the gpu. The end result has been gpu kernels that run at over 500 Gigaflops on
one nvidia Tesla C1060 card, thereby reaching close to practical peak. We can
confidently say that gpu computing is not just a vogue, it is truly an
irresistible trend in high-performance computing.