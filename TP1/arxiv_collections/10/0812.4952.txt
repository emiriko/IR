We present a practical and statistically consistent scheme for actively
learning binary classifiers under general loss functions. Our algorithm uses
importance weighting to correct sampling bias, and by controlling the variance,
we are able to give rigorous label complexity bounds for the learning process.
Experiments on passively labeled data show that this approach reduces the label
complexity required to achieve good predictive performance on many learning
problems.