The formalism of statistical mechanics can be generalized by starting from
more general measures of information than the Shannon entropy and maximizing
those subject to suitable constraints. We discuss some of the most important
examples of information measures that are useful for the description of complex
systems. Examples treated are the Renyi entropy, Tsallis entropy, Abe entropy,
Kaniadakis entropy, Sharma-Mittal entropies, and a few more. Important concepts
such as the axiomatic foundations, composability and Lesche stability of
information measures are briefly discussed. Potential applications in physics
include complex systems with long-range interactions and metastable states,
scattering processes in particle physics, hydrodynamic turbulence, defect
turbulence, optical lattices, and quite generally driven nonequilibrium systems
with fluctuations of temperature.