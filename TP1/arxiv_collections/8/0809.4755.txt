We discuss the property of a.e. and in mean convergence of the Kohonen
algorithm considered as a stochastic process. The various conditions ensuring
the a.e. convergence are described and the connection with the rate decay of
the learning parameter is analyzed. The rate of convergence is discussed for
different choices of learning parameters. We proof rigorously that the rate of
decay of the learning parameter which is most used in the applications is a
sufficient condition for a.e. convergence and we check it numerically. The aim
of the paper is also to clarify the state of the art on the convergence
property of the algorithm in view of the growing number of applications of the
Kohonen neural networks. We apply our theorem and considerations to the case of
genetic classification which is a rapidly developing field.