Differential privacy is a recent notion of privacy for statistical databases
that provides rigorous, meaningful confidentiality guarantees, even in the
presence of an attacker with access to arbitrary side information.
  We show that for a large class of parametric probability models, one can
construct a differentially private estimator whose distribution converges to
that of the maximum likelihood estimator. In particular, it is efficient and
asymptotically unbiased. This result provides (further) compelling evidence
that rigorous notions of privacy in statistical databases can be consistent
with statistically valid inference.