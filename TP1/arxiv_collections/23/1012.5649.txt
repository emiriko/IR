An information-theoretic framework known as integrated information theory
(IIT) has been introduced recently for the study of the emergence of
consciousness in the brain [D. Balduzzi and G. Tononi, PLoS Comput. Biol. 4,
e1000091 (2008)]. IIT purports that this phenomenon is to be equated with the
generation of information by the brain surpassing the information which the
brain's constituents already generate independently of one another. IIT is not
fully plausible in its modeling assumptions, nor is it testable due to severe
combinatorial growth embedded in its key definitions. Here we introduce an
alternative to IIT which, while inspired in similar information-theoretic
principles, seeks to address some of IIT's shortcomings to some extent. Our
alternative framework uses the same network-algorithmic cortical model we
introduced earlier [A. Nathan and V. C. Barbosa, Phys. Rev. E 81, 021916
(2010)] and, to allow for somewhat improved testability relative to IIT, adopts
the well-known notions of information gain and total correlation applied to a
set of variables representing the reachability of neurons by messages in the
model's dynamics. We argue that these two quantities relate to each other in
such a way that can be used to quantify the system's efficiency in generating
information beyond that which does not depend on integration, and give
computational results on our cortical model and on variants thereof that are
either structurally random in the sense of an Erdos-Renyi random directed graph
or structurally deterministic. We have found that our cortical model stands out
with respect to the others in the sense that many of its instances are capable
of integrating information more efficiently than most of those others'
instances.