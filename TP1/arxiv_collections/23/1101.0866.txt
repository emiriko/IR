The accumulation of {\it Swift} observed gamma-ray bursts (GRBs) gradually
makes it possible to directly derive a GRB luminosity function (LF) from
observational luminosity distribution, where however two complexities must be
involved as (i) the evolving connection between GRB rate and cosmic star
formation rate and (ii) observational selection effects due to telescope
thresholds and redshift measurements. With a phenomenological investigation on
these two complexities, we constrain and discriminate two popular competitive
LF models (i.e., broke-power-law LF and single-power-law LF with an exponential
cutoff at low luminosities). As a result, we find that the broken-power-law LF
could be more favored by the observation, with a break luminosity
$L_b=2.5\times10^{52}\rm erg s^{-1}$ and prior- and post-break indices
$\nu_1=1.72$ and $\nu_2=1.98$. For an extra evolution effect expressed by a
factor $(1+z)^{\delta}$, if the matallicity of GRB progenitors is lower than
$\sim0.1Z_{\odot}$ as expected by some collapsar models, then there may be no
extra evolution effect other than the metallicity evolution (i.e., $\delta$
approaches to be zero). Alternatively, if we remove the theoretical metallicity
requirement, then a relationship between the degenerate parameters $\delta$ and
$Z_{\max}$ can be found, very roughly,
$\delta\sim2.4(Z_{\max}/Z_{\odot}-0.06)$. This indicates that an extra
evolution could become necessary for relatively high metallicities.