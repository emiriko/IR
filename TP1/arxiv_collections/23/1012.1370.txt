The standard model of online prediction deals with serial processing of
inputs by a single processor. However, in large-scale online prediction
problems, where inputs arrive at a high rate, an increasingly common necessity
is to distribute the computation across several processors. A non-trivial
challenge is to design distributed algorithms for online prediction, which
maintain good regret guarantees. In \cite{DMB}, we presented the DMB algorithm,
which is a generic framework to convert any serial gradient-based online
prediction algorithm into a distributed algorithm. Moreover, its regret
guarantee is asymptotically optimal for smooth convex loss functions and
stochastic inputs. On the flip side, it is fragile to many types of failures
that are common in distributed environments. In this companion paper, we
present variants of the DMB algorithm, which are resilient to many types of
network failures, and tolerant to varying performance of the computing nodes.