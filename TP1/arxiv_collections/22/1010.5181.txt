The Augmented Lagrangian Method as an approach for regularizing inverse
problems received much attention recently, e.g. under the name Bregman
iteration in imaging. This work shows convergence (rates) for this method when
Morozov's discrepancy principle is chosen as a stopping rule. Moreover, error
estimates for the involved sequence of subgradients are pointed out.
  The paper studies implications of these results for particular examples
motivated by applications in imaging. These include the total variation
regularization as well as $\ell^q$ penalties with $q\in[1,2]$. It is shown that
Morozov's principle implies convergence (rates) for the iterates with respect
to the metric of strict convergence and the $\ell^q$-norm, respectively.