We investigate the concept of entropy in probabilistic theories more general
than quantum mechanics, with particular reference to the notion of information
causality recently proposed by Pawlowski et. al. (arXiv:0905.2992). We consider
two entropic quantities, which we term measurement and mixing entropy. In
classical and quantum theory, they are equal, being given by the Shannon and
von Neumann entropies respectively; in general, however, they are very
different. In particular, while measurement entropy is easily seen to be
concave, mixing entropy need not be. In fact, as we show, mixing entropy is not
concave whenever the state space is a non-simplicial polytope. Thus, the
condition that measurement and mixing entropies coincide is a strong constraint
on possible theories. We call theories with this property monoentropic.
Measurement entropy is subadditive, but not in general strongly subadditive.
Equivalently, if we define the mutual information between two systems A and B
by the usual formula I(A:B) = H(A) + H(B) - H(AB) where H denotes the
measurement entropy and AB is a non-signaling composite of A and B, then it can
happen that I(A:BC) < I(A:B). This is relevant to information causality in the
sense of Pawlowski et al.: we show that any monoentropic non-signaling theory
in which measurement entropy is strongly subadditive, and also satisfies a
version of the Holevo bound, is informationally causal, and on the other hand
we observe that Popescu-Rohrlich boxes, which violate information causality,
also violate strong subadditivity. We also explore the interplay between
measurement and mixing entropy and various natural conditions on theories that
arise in quantum axiomatics.