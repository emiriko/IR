We study the problem of estimating the time delay between two signals
representing delayed, irregularly sampled and noisy versions of the same
underlying pattern. We propose and demonstrate an evolutionary algorithm for
the (hyper)parameter estimation of a kernel-based technique in the context of
an astronomical problem, namely estimating the time delay between two
gravitationally lensed signals from a distant quasar. Mixed types (integer and
real) are used to represent variables within the evolutionary algorithm. We
test the algorithm on several artificial data sets, and also on real
astronomical observations of quasar Q0957+561. By carrying out a statistical
analysis of the results we present a detailed comparison of our method with the
most popular methods for time delay estimation in astrophysics. Our method
yields more accurate and more stable time delay estimates: for Q0957+561, we
obtain 419.6 days for the time delay between images A and B. Our methodology
can be readily applied to current state-of-the-art optical monitoring data in
astronomy, but can also be applied in other disciplines involving similar time
series data.