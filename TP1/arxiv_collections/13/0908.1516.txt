In meteorology, engineering and computer sciences, data assimilation is
routinely employed as the optimal way to combine noisy observations with prior
model information for obtaining better estimates of a state, and thus better
forecasts, than can be achieved by ignoring data uncertainties. Earthquake
forecasting, too, suffers from measurement errors and partial model information
and may thus gain significantly from data assimilation. We present perhaps the
first fully implementable data assimilation method for earthquake forecasts
generated by a point-process model of seismicity. We test the method on a
synthetic and pedagogical example of a renewal process observed in noise, which
is relevant to the seismic gap hypothesis, models of characteristic earthquakes
and to recurrence statistics of large quakes inferred from paleoseismic data
records. To address the non-Gaussian statistics of earthquakes, we use
sequential Monte Carlo methods, a set of flexible simulation-based methods for
recursively estimating arbitrary posterior distributions. We perform extensive
numerical simulations to demonstrate the feasibility and benefits of
forecasting earthquakes based on data assimilation. In particular, we show that
the forecasts based on the Optimal Sampling Importance Resampling (OSIR)
particle filter are significantly better than those of a benchmark forecast
that ignores uncertainties in the observed event times. We use the marginal
data likelihood, a measure of the explanatory power of a model in the presence
of data errors, to estimate parameters and compare models.