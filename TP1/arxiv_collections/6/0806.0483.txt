Information theory on a time-discrete setting in the framework of time series
analysis is generalized to the time-continuous case. Considerations of the
Roessler and Lorenz dynamics as well as the Ornstein-Uhlenbeck process yield
for time-continuous entropies a new possibility for the distinction of chaos
and noise. In the deterministic case an upper threshold of the joint
uncertainty in the limit of infinitely high sampling rate can be found and the
entropy rate can be calculated as a usual time derivative of the entropy. In a
three-dimensional representation the dependence of the joint entropy on space
resolution, discretization time step length and uncertainty-assessed time is
shown in a unified manner. Hence the dimension and the Kolmogorov-Sinai entropy
rate of any dynamics can be read out as limit cases from one single graph.