Cosmic-ray energy densities in central regions of starburst galaxies, as
inferred from radio and gamma-ray measurements of, respectively, non-thermal
synchrotron and neutral pion decay emission, are typically U_p = O(100)eV/cm3,
i.e. typically at least an order of magnitude larger than near the Galactic
center and in other non-very-actively star-forming galaxies. We first show that
these very different energy-density levels reflect a similar disparity in the
respective supernova rates in the two environments, which is not unexpected
given the supernova origin of (Galactic) energetic particles. As a consequence
of this correspondence, we then demonstrate that there is partial quantitative
evidence that the stellar initial mass function (IMF) in starburst nuclei has a
low-mass truncation at ~2M_sun, as predicted by theoretical models of turbulent
media, in contrast with the much smaller value of 0.1M_sun that characterizes
the low-mass cutoff of the stellar IMF in `normal' galactic environments.