The question whether a time series behaves as a random walk or as a station-
ary process is an important and delicate problem, particularly arising in
financial statistics, econometrics, and engineering. This paper studies the
problem to detect sequentially that the error terms in a polynomial regression
model no longer behave as a random walk but as a stationary process. We provide
the asymptotic distribution theory for a monitoring procedure given by a
control chart, i.e., a stopping time, which is related to a well known unit
root test statistic calculated from sequentially updated residuals. We provide
a functional central limit theorem for the corresponding stochastic process
which implies a central limit theorem for the control chart. The finite sample
properties are investigated by a simulation study.