We explore the effects of the deflagration to detonation transition (DDT)
density on the production of Ni-56 in thermonuclear supernova explosions (type
Ia supernovae). Within the DDT paradigm, the transition density sets the amount
of expansion during the deflagration phase of the explosion and therefore the
amount of nuclear statistical equilibrium (NSE) material produced. We employ a
theoretical framework for a well-controlled statistical study of
two-dimensional simulations of thermonuclear supernovae with randomized initial
conditions that can, with a particular choice of transition density, produce a
similar average and range of Ni-56 masses to those inferred from observations.
Within this framework, we utilize a more realistic "simmered" white dwarf
progenitor model with a flame model and energetics scheme to calculate the
amount of Ni-56 and NSE material synthesized for a suite of simulated
explosions in which the transition density is varied in the range 1-3x10^7
g/cc. We find a quadratic dependence of the NSE yield on the log of the
transition density, which is determined by the competition between plume rise
and stellar expansion. By considering the effect of metallicity on the
transition density, we find the NSE yield decreases by 0.055 +/- 0.004 solar
masses for a 1 solar metallicity increase evaluated about solar metallicity.
For the same change in metallicity, this result translates to a 0.067 +/- 0.004
solar mass decrease in the Ni-56 yield, slightly stronger than that due to the
variation in electron fraction from the initial composition. Observations
testing the dependence of the yield on metallicity remain somewhat ambiguous,
but the dependence we find is comparable to that inferred from some studies.