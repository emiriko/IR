We perform a suite of simulations of cooling cores in clusters of galaxies in
order to investigate the effect of the recently discovered heat flux buoyancy
instability (HBI) on the evolution of cores. Our models follow the
3-dimensional magnetohydrodynamics (MHD) of cooling cluster cores and capture
the effects of anisotropic heat conduction along the lines of magnetic field,
but do not account for the cosmological setting of clusters or the presence of
AGN. Our model clusters can be divided into three groups according to their
final thermodynamical state: catastrophically collapsing cores, isothermal
cores, and an intermediate group whose final state is determined by the initial
configuration of magnetic field. Modeled cores that are reminiscent of real
cluster cores show evolution towards thermal collapse on a time scale which is
prolonged by a factor of ~2-10 compared with the zero-conduction cases. The
principal effect of the HBI is to re-orient field lines to be perpendicular to
the temperature gradient. Once the field has been wrapped up onto spherical
surfaces surrounding the core, the core is insulated from further conductive
heating (with the effective thermal conduction suppressed to less than 1/100th
of the Spitzer value) and proceeds to collapse. We speculate that, in real
clusters, the central AGN and possibly mergers play the role of "stirrers,"
periodically disrupting the azimuthal field structure and allowing thermal
conduction to sporadically heat the core.