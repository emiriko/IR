In a number of natural and social systems, the response to an exogenous shock
relaxes back to the average level according to a long-memory kernel $\sim
1/t^{1+\theta}$ with $0 \leq \theta <1$. In the presence of an epidemic-like
process of triggered shocks developing in a cascade of generations at or close
to criticality, this "bare" kernel is renormalized into an even slower decaying
response function $\sim 1/t^{1-\theta}$. Surprisingly, this means that the
shorter the memory of the bare kernel (the larger $1+\theta$), the longer the
memory of the response function (the smaller $1-\theta$). Here, we present a
detailed investigation of this paradoxical behavior based on a
generation-by-generation decomposition of the total response function, the use
of Laplace transforms and of "anomalous" scaling arguments. The paradox is
explained by the fact that the number of triggered generations grows
anomalously with time at $\sim t^\theta$ so that the contributions of active
generations up to time $t$ more than compensate the shorter memory associated
with a larger exponent $\theta$. This anomalous scaling results fundamentally
from the property that the expected waiting time is infinite for $0 \leq \theta
\leq 1$. The techniques developed here are also applied to the case $\theta >1$
and we find in this case that the total renormalized response is a {\bf
constant} for $t < 1/(1-n)$ followed by a cross-over to $\sim 1/t^{1+\theta}$
for $t \gg 1/(1-n)$.