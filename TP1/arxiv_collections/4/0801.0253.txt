We consider words as a network of interacting letters, and approximate the
probability distribution of states taken on by this network. Despite the
intuition that the rules of English spelling are highly combinatorial (and
arbitrary), we find that maximum entropy models consistent with pairwise
correlations among letters provide a surprisingly good approximation to the
full statistics of four letter words, capturing ~92% of the multi-information
among letters and even "discovering" real words that were not represented in
the data from which the pairwise correlations were estimated. The maximum
entropy model defines an energy landscape on the space of possible words, and
local minima in this landscape account for nearly two-thirds of words used in
written English.